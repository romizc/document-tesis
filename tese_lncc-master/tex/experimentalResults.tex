\chapter{Implementation and Experimental Results}
\label{chapter_Experimental_Results}

This Chapter is dedicated to describe the experimental validation of the methodology presented in the previous Chapter. Extensive results and analysis are presented to explore each of the steps in the proposed methodology. In particular, we focus on both the robustness of the domain characterization and in the accuracy of the results obtained from the spatio--temporal predictive queries. In each case, we employ alternative methods as baseline for comparing the performance of the proposed approach.

Several aspects of the methodology have been implemented in the form of a software called ``Spatio-Temporal Tool for Time Series Analysis'' or SPT-TSA. This software is the subject of the next section.
%\Fab{Me parece que a seção 5.1 trata do Design da solução e não de Results e Analysis. Acho que vc poderia mudar o Titlo do Capitulo para "Implementation and results". E dividir 5.1 Implementation, 5.2 Results}
%This Chapter is dedicated to describe the architectural design of a package called ``Spatio-Temporal Tool for Time Series Analysis'' or SPT-TSA, the package was implemented in Python 3 and its purpose is to perform the computational elements involved in the methodology to acquire data in order to validate our hypothesis. 

%In Chapter \label{}, Figure \label{} shows the workflow of the methodology proposed. 

\section{Spatio Temporal Framework for Time Series Analysis}       

We will describe the architectural design of SPT-TSA, a Python 3 package built to work with spatio-temporal data and predictive models. This package was developed as an implementation of a computational solution to execute the methodology described in the previous Chapter, but it can be extended to be used for other similar purposes in the field of spatio-temporal analysis.

%In this Section we describe the development and implementation of a computational solution to execute the proposed methodology. 

\subsection{SPTA-TSA Workflow}

When dealing with spatio-temporal data and in particular time-series data, it is possible to find libraries and packages to perform operations for data manipulation, and for time-series analysis, particularly in the Python and R programming languages. Our software SPTA-TSA was designed to leverage widely known libraries and packages developed for Python 3, such as Pandas, Scikit-Learn, DTW, Keras, TensorFlow, Pickle, Numpy, MatplotLib, among others.

Figure \ref{Fig:Steps-Data-Transformation} shows the tasks included on each step, as well as the libraries and packages responsible for the manipulation and analysis of spatio-temporal data.

%The application was implemented using Python 3.6, to perform several operations we rely on 

\begin{figure}[tp]
	%\begin{minipage}[b]{0.8\textwidth}
	\centering
	\includegraphics[scale=0.25, angle=90]{../Figures/workflow_data_operations}
	\caption{Steps for Dataset Transformation and Generated Products.}	
	\label{Fig:Steps-Data-Transformation}	 		
\end{figure}

% Figure Description

\subsection{Class Diagrams}
\label{Sec:SPT-TSAClassDiagrams}

To better describe the functionality and architectural design of SPT-TSA, we present two class diagrams, each showing a different, noteworthy aspect of the implementation. Figure \ref{Fig:DiagramClasess-Region} shows a class diagram concerning the implementation of spatio-temporal regions and operations that can be applied to them. The diagram indicates that spatio-temporal and spatial regions are built upon a common domain region, on which operations can be performed. The operations are special functions (hereby called function regions) that can be applied to either spatial or spatio-temporal regions, and can produce them as output. This means that, in SPT-TSA, these operations are transformations of domain regions. Additional properties of spatio-temporal regions, such as grouping from partitioning schemes and the scaling of temporal data, are achieved using decorators.

\begin{figure}[tp]
	%\begin{minipage}[b]{0.8\textwidth}
	\centering
	\includegraphics[scale=0.40, angle=90]{../Figures/SPT-TSA-RegionClasses}
	\caption{Class Diagram -- Region Class.}	
	\label{Fig:DiagramClasess-Region}	 		
\end{figure}

A brief description of each class is provided:

\begin{itemize}
	\item \texttt{Point}: a simple implementation of a 2D position using a \texttt{namedtuple}.
	
	\item \texttt{BaseRegion}: The base class for all regions. It is a wrapper of a \texttt{numpy.ndarray} object, which constrains the dimensions of the region inside a 2D rectangle. In addition to providing basic functions such as saving a dataset to persistent storage, it allows the iteration of the domain points contained within the 2D boundary, using Point instances.
	
	\item \texttt{PartitionRegion}: Describes the application of a partitioning scheme to divide a spatio-temporal region into groups, by storing the membership of each point in the region to a group. Note that it is kept separate from \texttt{DomainRegion}, because it is not meant to accept function regions.
	
	\item \texttt{DomainRegion}: The parent class of both spatial and spatio-temporal regions, and also the operand and result of function regions. This abstract implementation has common functionalities for both 2D and 3D arrays, and can be used to create new spatial and spatio-temporal regions that are of a particular instance. Since function regions transform a domain region into a new region, we allow subclasses to determine which new instance to create by leveraging polymorphism. For example, a function region acting on an instance of \texttt{SpatialRegion} may produce a new \texttt{SpatialRegion} instance using the \texttt{new\_sp\_region} method declared in this class. However, that same function region acting on an instance of \texttt{SpatialCluster}, using the same \texttt{new\_sp\_region} method, will produce an instance of \texttt{SpatialCluster} instead, due to the polymorphic method call. 
	
	\item \texttt{SpatialRegion}: The 2D implementation of \texttt{DomainRegion}, it stores a value in each point. Iterations of the points in the domain will yield the values for each point.
	
	\item \texttt{SpatioTemporalRegion}: The 3D implementation of \texttt{DomainRegion}, it stores an array in each point, representing a time series. Iterations of the points in the domain will yield the series for each point.
	
	\item \texttt{FunctionRegion}:
	%\Fab{esclareça melhor o objetivo dessa classe. Vc diz que ela implementa uma função a cada ponto da região, mas o objeto é um por Região...}
	A special type of \texttt{DomainRegion} used to transform \texttt{DomainRegion} instances, by accepting a \texttt{DomainRegion} instance as input and producing another \texttt{DomainRegion} instance as output, potentially of a different type. It serves as a common class for \texttt{FunctionRegionScalar} (produces a \texttt{SpatialRegion} instance as output) and \texttt{FunctionRegionSeries} (produces a \texttt{SpatioTemporalRegion} instance as output). The transformation of the input is achieved by storing a Python function at each point of the function region, and applying each function to the corresponding element in the operand region. To represent this application, consider a spatial region $S_{(m,n)}$ and a function region $F_{(m,n)}$, we then have $F(S) = \left\{ f_{ij}(s_{ij}); (i, j) \in [0, m]\times[0, n] \right\}$. See \texttt{FunctionRegionScalar} for practical examples.
	%a Python function at each point of the region. These functions are stored in a similar way as a \texttt{SpatialRegion} but using a \texttt{numpy.dtype} of value `object' in its internal 2D array.
	
	The implementation uses the Visitor pattern, where the \texttt{DomainRegion} is the visitor that `visits' the function region to retrieve the corresponding function during an iteration of points. The benefit of the visitor approach is that the same \texttt{FunctionRegion} can be applied to different subclasses of \texttt{SpatialRegion} and \texttt{SpatioTemporalRegion} without knowledge of the particular subclasses, while still producing different and expected outputs. A limitation of this approach is that only one function region can be applied to a given region at the same time, otherwise the iterator of the region will be used twice and become corrupted.
	
	%of region. stores a Python function at each point of its region.  %\Fab{ou Region?}. It leverages a \texttt{numpy.ndarray} that has a \texttt{numpy.dtype} of value `object'. The generic implementation allows for different functions at each point, but additional subclasses simplify usage by assuming the same function in every point. 
	
	\item \texttt{FunctionRegionScalar}%\Fab{Scalar?}
	: A subclass of \texttt{FunctionRegion} that produces a subclass of \texttt{SpatialRegion} when applied to a \texttt{DomainRegion}, regardless of the input domain being spatial or spatio-temporal. The implementation will call the \texttt{new\_sp\_region} method of the operand polymorphically to produce the desired output.
	
	As a simple example, consider the problem of averaging each series in a spatio-temporal region, by using the function \texttt{numpy.mean} to the series at each point of the region. This can be achieved by creating an instance of \texttt{FunctionRegionScalar} so that each point of its internal 2D array contains the function \texttt{numpy.mean} itself, and applying this function region to the spatio-temporal region. A more complicated example involves having a special \texttt{FunctionRegionScalar} instance that applies training functions to spatio-temporal regions in order to produce, as output, a region with predictive models at each point.
	
	\item \texttt{FunctionRegionSeries}: Analogous to \texttt{FunctionRegionScalar} but will always produce a subclass of \texttt{SpatioTemporalRegion} by calling the \texttt{new\_spt\_region} method of the input. It requires to know the length of the output series in advance in order to allocate the output 3D array.
	
	\item \texttt{ScaleFunction}: Produces an instance of \texttt{SpatioTemporalScaled} (see below) when applied to a \texttt{SpatioTemporalRegion}.
	
	\item \texttt{SpatialDecorator}: Implementation of the Decorator pattern for a \texttt{SpatialRegion}.
	
	\item \texttt{SpatialCluster}: A decorated version of \texttt{SpatialRegion} that provides support for describing clusters obtained from a partitioning scheme. Each instance of \texttt{SpatialCluster} represents one group only (a group index is used), so $k$ instances would be required to represent the output of a partitioning scheme of $k$ groups. Iterations of \texttt{SpatialCluster} will only yield the points that are members of the corresponding group, and attempts to read the value of a point outside the group result in error. This effect is achieved by keeping the same underlying data for all instances (no additional copies are made), while also using an instance of \texttt{PartitionRegion} and the corresponding group index to decorate some of the relevant methods.
	
	\item \texttt{SpatioTemporalDecorator}: Implementation of the Decorator pattern, but this time for a \texttt{SpatioTemporalRegion}.
	
	\item \texttt{SpatioTemporalCluster}: A decorated version of \texttt{SpatioTemporalRegion} that provides support for describing clusters obtained from a domain partitioning technique. This class uses multiple inheritance to also extend \texttt{SpatialCluster} to get cluster behavior, but at the same time work with a 3D array and provide a series at each point.
	
	\item \texttt{SpatioTemporalScaled}: A decorated version of \texttt{SpatioTemporalRegion} that represents the scaled version of a spatio-temporal region. The scaling process is performed by a \texttt{ScalingFunction}, which will iterate each point to scale independently the corresponding series. A series is scaled by finding its minimum and maximum values so that the new values are in the $[0, 1]$ interval. These minimum and maximum values are stored as \texttt{SpatialRegion} instances inside the scaled region, so that the original values can be restored at a later point. Since both \texttt{SpatioTemporalCluster} and \texttt{SpatioTemporalScaled} are decorators, it is possible to chain these decorators, arriving at the expected output (a cluster representing a group of scaled series at each point), even if the decorators are applied in different order. Also, restoring the scaling in that example will yield a \texttt{SpatioTemporalCluster} again.
\end{itemize}

The second class diagram is shown in Figure \ref{Fig:DiagramClasess-Models}. %\Fab{Full stop}
The main focus is now the analysis of the forecast error. Here we find classes related to the training of predictive models, their forecasting of future values of the predictive variable and the subsequent calculation of the forecast error, when the actual values are available. The descriptions provided here are for the relevant ARIMA predictive models. Other predictive models used as baseline for comparison follow a similar class structure of classes.

\begin{figure}[tp]
	%\begin{minipage}[b]{0.8\textwidth}
	\centering
	\includegraphics[scale=0.36, angle=90]{../Figures/SPT-TSA-ModelsClasses}
	\caption{Class Diagram -- Model Class.}	
	\label{Fig:DiagramClasess-Models}	 		
\end{figure}

\begin{itemize}
	\item \texttt{ModelRegion}: Base class for the different predictive models supported by SPT-TSA. A model will produce, for each point in a spatio-temporal region, a series that represents the predicted values of the predictive variable. This is achieved by extending the behavior of \texttt{FunctionRegionSeries}, where the length of the series represents to the size of the forecast. If, for example, we pass an instance of \texttt{SpatioTemporalRegionScaled} as an operand, then the output is also scaled, and the  original scaling information can be used to recover the desired forecast series.
	
	\item \texttt{ModelTrainer}: Base class for training predictive models supported by SPT-TSA. The models are trained by passing a training spatio-temporal region containing, for each point, a training series (same length for all points in the region). The implementation overrides the behavior of \texttt{FunctionRegionScalar} to produce an instance of \texttt{ModelRegion} instead of \texttt{SpatialRegion}. The exact class of the instance and its properties (resulting model parameters) will depend on the subclasses of \texttt{ModelTrainer} and how they process the \texttt{model\_params} input.
	
	\item \texttt{TrainerArimaPDQ}: A model trainer that will use, for each point, the class \\ \texttt{statsmodels.tsa.arima.model.ARIMA} to fit an ARIMA model using a training series and a $(p, d, q)$ tuple. The result of applying this function to a training region is an instance of \texttt{ModelRegionArima}.
	
	\item \texttt{TrainerAutoArima}: A model trainer that will use, for each point, \\
	\texttt{pmdarima.arima.auto\_arima} and a training series to determine the values for the $(p, d, q)$ tuple that maximize the AIC metric. The trainer will then use the training function of \texttt{TrainerArimaPDQ} to fit the resulting ARIMA model.
	
	\item \texttt{ModelRegionArima}: Represents the application of ARIMA as predictive model. It is a region that has an instance of \texttt{statsmodels.tsa.arima.model.ARIMAResults} at each point (the same instance could be referenced by many points if needed). When applied to a spatio-temporal region, it will create a new spatio-temporal region that holds, for each point, its corresponding forecast series given by \\
	\texttt{statsmodels.tsa.arima.model.ARIMAResults.forecast()}.
	
	\item \texttt{ErrorRegion}: A simple decoration of \texttt{SpatialRegion} that is meant to hold, for each point, a value representing the forecast error of some model. The decoration provides some additional functions for combining the errors in each point into a single metric, for example using the RMSE.
	
	\item \texttt{MeasureForecastError}: Calculates the associated forecast errors between an observation region (an observed series in each point) and a forecast region (a forecast series in each point). It leverages the implementation of \texttt{FunctionRegionScalar} to operate in each point with a given error metric, for example RMSE. The result of applying this function region to a forecast region is an instance of \texttt{ErrorRegion}.
	
	\item \texttt{ErrorAnalysis}: A class that provides methods to calculate forecast errors using \texttt{MeasureForecastError} as a helper class. It can either appply the helper implementation directly given an entire forecast region, or it can create a forecast region by repeating a single forecast series over each point of the observation region. This is useful when using the predictive model of a representative to create the same forecast for an entire region (\texttt{SpatioTemporalRegion}) or an entire group (\texttt{SpatioTemporalCluster}).
	
	\item \texttt{ForecastAnalysis}: Provides several strategies for training predictive models and evaluating their associated forecasts. It supports the training strategies described in the methodology, such as training a model at every point (naive approach), or training a model only at the representatives of each group. The second approach is designed so that each group is treated independently using \texttt{SpatioTemporalCluster}, so that the actual implementation remains unaware of clusters: it uses the model of a single given point (meant to be the representative) and repeats it over the entire region using point iteration (will iterate only over the members of the group). This class also provides additional methods to calculate several associated errors, including the errors obtained when the forecast of every single model is repeated independently over the entire region. This is what allows the calculation of the `Model Composition with Minimal Local Error' and `Model Composition with Maximum Error' described in Section \ref{Sec:ModelRepresentatives}, they are found by exhausting every point in the region. Since the models at every point can be analyzed independently, this calculation has been parallelized to improve efficiency.
\end{itemize}

% The two main components of the package are two classes: Region and Model. The Region Class is responsible for the following tasks:
% \begin{itemize}
% 	\item Load dataset, we 
% 	\item Crop data selection, the dataset 
% 	\item DTW Matrix 
% 	\item Compute Partition profile 
% 	\item Store Pickle templates
% \end{itemize} 

\section{Experiments and Results}

In this section we describe the experiments and discuss the results obtained.

\subsection{Case Study: Temperature Forecasting}

The case study considered is the forecast of surface temperature, and for that we consider the CFSR (Climate Forecast System Reanalysis) dataset. It is described extensively in \cite{Saha2010}, here we provide a summary. It is a third generation reanalysis product; it consists of a high-resolution, global surface system, of the coupled ocean-atmosphere-terrestrial surface-sea ice system, covering the period from January 1979 to November 2015. The CFSR includes: (1) coupling of atmosphere and ocean during the generation of kick-off fields every 6 hours; (2) an interactive model of sea ice and; (3) assimilation of satellite radiance from the statistical interpolation scheme by grid points throughout the period. The resolution of the CFSR's global atmosphere is $\sim 38$ km. The global ocean is $0.25$\textdegree at the equator, extending to $0.5$\textdegree, in the tropics. %A more detailed description of the data can be found in cite{Saha2010}. 

% Use case and Temperature dataset
%We are interested the information about the temperature $T$ from the original dataset, for this variable the data acquired are time-series with $4$ readings per day (every 6 hours) for each geographical coordinate $(x,y)\in ([278, 348], [10, -60])$, with a resolution of $\sim 38$ km or $0.5$\textdegree. A similar region was extracted from the same dataset in a previous analysis \cite{Souto2018}.

In the next subsection, we describe the pre-processing of the dataset and the overall preparation for our proposed methodology. Then, we show how each of the steps are applied to the use case of temperature forecasting, with the corresponding presentation and analysis of the results of each step.

\subsection{Dataset Preparation and Computational Environment}
\label{sec:DatasetPreparation}

To validate the proposed methodology, we are interested in the information about the temperature $T$ from the original dataset, for this variable the data acquired are time-series with $4$ readings per day (every 6 hours) for each geographical coordinate $(x,y)\in ([278, 348], [10, -60])$, with a resolution of $\sim 38$ km or $0.5$\textdegree. This spatial subset comprises the Brazilian territory, and has been studied before in \cite{Souto2018}. Additionally, we restrict our use case to the last year collected. Thus, we can denote the dataset for our region of interest as $\mathcal{D} = [ 365\times 4, 285.5:330, 7.5:-37]$ (there are 4 daily measurements with 6 hour interval during one year, the spatial coordinates denote the Brazilian territory). 

%we consider a subset of the dataset which comprises the Brazilian territory, while the temperature measurements corresponds to the last year collected. Thus, we can denote the dataset for our region of interest as $\mathcal{D} = [ 365\times 4, 285.5:330, 7.5:-37]$ (there are 4 daily measurements with 6 hour interval during one year, the spatial coordinates denote the Brazilian territory). 

In total there are 8100 time-series representing the temperature variation for one year. Each time-series in the dataset consists of 1260 values, four readings per day. To reduce daily noise, we calculate the mean of each four daily values to produce a single average daily value; this results in 365 time-units. 

To further pre-process the data, all the time-series are scaled so the values are bounded in the $[0, 1]$ interval. This scaling process is done independently for each point: we find the minimum and maximum values, then apply a simple scaling function $ t_{scaled} = \frac{t - t_{min}}{t_{max} - t_{min}}$. There are many reasons to apply this scaling function, most importantly is to allow for offsets when comparing similar temporal trends. This will be specially useful when using a predictor to estimate the future values of a predictive variable: this way the same predictor can produce different predicted values for each point in the region after applying the inverse scaling function corresponding to each point, even though all the predictions come from the same scaled output.

\subsection{Calculating the DTW matrix}
%TODO time in seconds for DTW!
In Section \ref{Sec:DomainPartitioning}, we argue for the choice of DTW as the shape-based similarity measure for the partitioning schemes. Here, we describe the computation of the DTW matrix for our entire region of interest, as part of the off-line process. This compute-intensive process needs to be performed only once for each region of interest (bounded not only by the Brazilian territory but also by the chosen temporal slice). It can be performed in parallel using a multi-core machine with satisfactory speedup gain. To achieve parallelism, we leveraged the $multiprocessing$ library from Python to split the tasks of calculating the DTW for each pair points. To optimize speedup, the dataset was loaded into shared readable memory between processes, while allowing a shared writeable memory for writing the DTW distance. The time used to compute the DTW matrix for 8100 univariate time-series with 365 temperature readings was approximately 90000 seconds (25 hours).
%\Fab{Update}$xxxx $ seconds. 

%\Fab{Lembrete: discutir DTW na parte de Referencia teorico}
Once the DTW matrix is computed, it is saved to persistent storage for later retrieval. Afterwards, we are ready to apply a clustering algorithm to find groups based on the measure similarity. 
%\Fab{A frase a seguir pode ser melhor escrita: the retrieval of distance values between points...} OK
The retrieval of distance values between points is simplified to finding rows or columns in the DTW matrix, thereby significantly reducing the computational cost of algorithms such as $k$-medoids.

Here, we provide a couple of examples that highlight the need for computing the DTW matrix for efficiency, by comparing execution times on the same reference hardware. Running our implementation of $k$--medoids without the DTW matrix involves calculating pair-wise DTW distances between candidate medoids and other points until the algorithm converges. Let's first consider a subset of the spatio-temporal region described in the previous section, with only 70 time-series ($7 \times 10$ spatial region). If the DTW matrix is used, running $k$--medoids with $k=8$ on this smaller region takes less than one second to execute three iterations with a total of 25 swaps in the candidate medoids; as opposed to 292 seconds when the DTW matrix is not used. The difference is more dramatic when running $k$--medoids with $k=24$ over the entire region of interest with 8100 time-series: with the DTW matrix, it  takes 72.5 seconds to execute six iterations with a total of 271 swaps; without the DTW matrix only the first swap took over 4400 seconds. We need not only to execute $k-$-medoids once but many times with different partitioning schemes, this would be unfeasible without the DTW matrix calculated beforehand.

In Figure \ref{Fig:DTW-Distance}, we can observe the variations in the DTW distances for the Brazilian region, using two plots. The DTW distances are actually calculated as a $[x\_len * y\_len, x\_len * y\_len]$ symmetric matrix. Here we use two plots to highlight the distances between the point $(0, 0)$ and all other points (left image), and between the center of the domain and all other points (right image). For the first plot, we retrieve the first row of the DTW matrix, and arrange the data to fit into a 2D grid that represents the region of interest using indices that match the spatial location of the time series. The distances are represented using colors obtained by applying the default \texttt{viridis} colormap from the \texttt{matplotlib} Python package. With this colormap, darker colors represent lower values of the DTW distance (black means 0, which is the distance of a point with itself), and the maximum value of the DTW distance is displayed as a bright yellow. These maximum values were $3.19$ (left) and $3.80$ (right) for our dataset with the scaling of the time-series.

%it is possible to observe variations in DTW distances from all the time-series to the point $(0,0)$ (left image) and to the center of the domain (right image); darker colors represent smaller values of the DTW distance.

\begin{figure}[h!]
	
	\centering
	\subfloat{%[DTW distance from the point $(0,0)$ to the other elements.]{\label{Fig:DTW-Distance0}
		\centering
		\includegraphics[width=0.45\linewidth]{../Figures/distances_0_0_whole_real_brazil.pdf}
	}
	%no space
	\hfill
	\subfloat{%[DTW distance from the center to the other elements.]{\label{Fig:DTW-DistanceCenter}
		\centering
		
		\includegraphics[width=0.45\linewidth]{../Figures/distances_center_whole_real_brazil.pdf}
	}
	
	\caption{DTW Distance of the time-series in the Brazilian region: axes represent spatial indices of the time series, color represents the value of the distance.}
	\label{Fig:DTW-Distance}
\end{figure}

\subsection{Analyzing the Domain Partitioning}
\label{Sec:AnalyzeDomainPartitioning}

In this Section we describe the experiments performed to validate the strategies adopted for the domain partitioning. We consider two partitioning techniques: $k$--medoids based and regular partitioning; the first groups elements of the domain according to the DTW similarity measure, whereas the second is a partitioning technique based on the geometry of the domain (see Section \ref{Sec:DomainPartitioning} for details). In both approaches, we consider the existence of representative elements for each group in the partition, medoid and centroid respectively. 

%TODO when this hypothesis is validated, refer to Section \ref{Sec:AnalyzeDomainPartitioning}
A hypothesis we want to highlight is that it should be preferable to use a partitioning technique that considers grouping domain elements based on the similarity of their temporal evolution, rather than creating groups just according to a regular division of the domain geometry. To verify this, we compare the intra-cluster sum of both partitioning techniques for several values of $k$.

%TODO cleanup
Another important aspect to evaluate in $k$--medoids is the choice of the number of partitions ($k$), for this we use three approaches: elbow method, silhouette index and the *point with minimum curvature* inflection point using a smooth spline (see Section \ref{sub:domain_number_groups} for a description of these methods).

\subsubsection{Evaluating Partitioning Quality}
\label{Sec:EvaluatingPP}

% Considering two types of partition profiles based on the domain geometry and shape based 
For the two partition techniques, $k$--medoids based and regular partitioning, we vary the number of groups from 2 to 150. As mentioned in Section \ref{Sec:DomainPartitioning}, the clustering algorithm $k$--medoids involves a similarity measure. We are using the DTW distance, due to its much improved alignment based on shape. And the regular partitioning is a method based on the domain geometry, i.e. divide in regions of the same size and shape.

% Considering varying k partitions
% TODO seed based? cita?
% TODO without penalty?
The $k$--medoids approach is a seed based algorithm, this means that the performance is dependent on initial cluster center selection and the optimal number of clusters. The convergence of this algorithm primarily depends on the initialization phase. It produces different clustering results for different values of $k$. In addition, increasing $k$ without penalty will always reduce the error in the resulting clustering, to the extreme case of zero error if each data point is considered its own cluster (i.e., when $k$ equals the number of data points) \cite{HastieTF2009}. 

In Table \ref{Table:TotalSumRegularkMedoids}, we show the Total Sum of Intra-Cluster distances for both domain partitioning techniques, with $k$ vaying from 2 to 150, graphically represented in Figure \ref{Fig:TotalSum-RegularKmedoids}. 
\begin{table}[h]
	\centering
	\tiny
	\begin{tabular}{|c|r|r|c|r|r|c|r|r|}
		\hline
		$k$  & $k$--Medoids & Regular & $k$ & $k$--Medoids & Regular & $k$ & $k$--Medoids & Regular \\ \hline
		2  & 12468.405 & 13190.136 &  52 & 8645.736 & 10032.124 & 102 & 7796.861 &  8855.428 \\
		4  & 11677.377 & 13115.736 &  54 & 8606.070 &  9485.886 & 104 & 7772.689 &  8706.568 \\
		6  & 11218.553 & 12546.427 &  56 & 8566.783 &  9480.166 & 106 & 7752.124 & 12067.876 \\
		8  & 10861.431 & 12517.237 &  58 & 8518.216 & 11466.201 & 108 & 7720.876 &  8404.685 \\
		10 & 10556.589 & 12144.754 &  60 & 8470.036 &  9360.978 & 110 & 7705.371 &  8359.722 \\
		12 & 10320.748 & 11537.237 &  62 & 8432.220 & 11922.456 & 112 & 7676.569 &  8470.927 \\
		14 & 10162.890 & 12038.902 &  64 & 8391.178 &  9238.163 & 114 & 7657.374 &  9012.854 \\
		16 & 10016.352 & 11185.784 &  66 & 8352.620 &  9222.670 & 116 & 7631.154 &  9513.241 \\
		18 &  9874.266 & 11011.460 &  68 & 8315.007 &  9769.658 & 118 & 7612.637 & 11943.777 \\
		20 &  9755.024 & 10864.368 &  70 & 8278.471 &  9148.631 & 120 & 7587.633 &  8276.233 \\
		22 &  9623.499 & 11663.244 &  72 & 8232.783 &  9011.589 & 122 & 7570.840 & 11900.903 \\
		24 &  9539.343 & 10613.759 &  74 & 8203.346 & 11645.734 & 124 & 7547.208 & 10152.428 \\
		26 &  9445.446 & 11741.562 &  76 & 8169.848 &  9864.922 & 126 & 7526.430 &  8218.706 \\
		28 &  9358.812 & 10467.305 &  78 & 8130.028 &  9243.270 & 128 & 7510.721 &  8428.220 \\
		30 &  9279.477 & 10250.783 &  80 & 8101.370 &  8879.863 & 130 & 7485.228 &  8341.508 \\
		32 &  9197.485 & 10305.462 &  82 & 8069.011 & 11478.049 & 132 & 7465.314 &  8149.479 \\
		34 &  9135.674 & 11595.505 &  84 & 8039.939 &  8947.657 & 134 & 7447.742 & 11768.667 \\
		36 &  9083.476 & 10081.895 &  86 & 8014.447 & 11439.713 & 136 & 7431.451 &  8247.420 \\
		38 &  9018.006 & 11685.706 &  88 & 7985.764 &  8719.332 & 138 & 7406.728 &  9136.191 \\
		40 &  8969.207 &  9903.558 &  90 & 7955.133 &  8654.838 & 140 & 7390.761 &  8083.724 \\
		42 &  8902.609 &  9865.422 &  92 & 7922.941 & 10004.969 & 142 & 7366.944 & 11675.846 \\
		44 &  8856.496 &  9990.224 &  94 & 7906.157 & 12251.765 & 144 & 7352.245 &  8126.600 \\
		46 &  8795.638 & 11808.071 &  96 & 7884.088 &  8642.817 & 146 & 7333.966 & 11621.432 \\
		48 &  8740.765 &  9653.982 &  98 & 7855.217 &  8773.346 & 148 & 7311.080 &  9715.298 \\
		50 &  8698.159 &  9605.287 & 100 & 7822.593 &  8525.302 & 150 & 7293.661 &  7932.553 \\ \hline
		
	\end{tabular}
	\caption{Total Sum of Intra Cluster Distances of $k$--Medoids and Regular Partitioning Techniques for $k = \{2, \ldots, 150\}$.}
	\label{Table:TotalSumRegularkMedoids}
\end{table}

\begin{figure}[h]
	%	\begin{minipage}[b]{0.8\textwidth}
	\centering
	\includegraphics[scale=0.45]{../Figures/Scaled-TotalSum-RegularKmedoids}
	\caption{Total Sum of Intra Cluster Distances of $k$--Medoids and Regular Partitioning Techniques.}
	\label{Fig:TotalSum-RegularKmedoids}
	%	\end{minipage}
\end{figure}

\subsubsection{Selecting $k$}
\label{Sec:Selectk}

%TODO methodology vs experiment (move initial explanation to previous chapter, remove silhouette details)
%TODO use \ref and refer to methodology here

% Methods to select k
The performance of the $k$--Medoids algorithm is dependent on the optimal value of $k$. The automatic detection of $k$ is very challenging for time-series data set. The choice of $k$ is often ambiguous, with interpretations depending on the shape and scale of the distribution of the time--series in a dataset \cite{Liao2005}. An optimal choice of $k$ should strike a balance between maximizing the compression of the data using a single cluster, while maximizing the accuracy when assigning each data point to its own cluster. 

% TODO Elbow Method and Results
One common method of choosing the appropriate cluster solution is to compare the Within-Sum-of-Squares (WSS) for a number of cluster solutions. Thus, WSS can be seen as a global measure of error. In general, as the number of clusters increases, the WSS should decrease because clusters are, by definition, smaller. A plot of the WSS against a series of sequential cluster levels can provide a useful graphical way to choose an appropriate cluster level. An appropriate cluster solution could be defined as the solution at which the reduction in WSS slows dramatically. This produces an ``elbow'' in the plot of WSS against cluster solutions. For our dataset we find an ``elbow'' at the 4 cluster solution suggesting that solutions $> 4$ do not have a substantial impact on the total SSE, Figure \ref{Fig:SSE-kMedoids} shows this result.

% TODO bigger fonts
\begin{figure}[h]
	%	\begin{minipage}[b]{0.8\textwidth}
	\centering
	\includegraphics[scale=0.5]{../Figures/Elbow-Kmedoids}
	\caption{Elbow Method to find the optimal $k$ for the $k$--Medoids Approach.}
	\label{Fig:SSE-kMedoids}
	%	\end{minipage}
\end{figure}

% TODO Silhouette Analysis and Results
When performing a Silhouette Analysis, a way to measure how close each point in a cluster is to the points in its neighboring clusters. We expect to find the optimum value for $k$ during $k$-means clustering. Silhouette values lies in the range of $[-1, 1]$. A value of $+1$ indicates that the sample is far away from its neighboring cluster and very close to the cluster its assigned. Similarly, value of $-1$ indicates that the point is close to its neighboring cluster than to the cluster its assigned. And, a value of $0$ means its at the boundary of the distance between the cluster. Value of $+1$ is ideal and $-1$ is least preferred. Hence, higher the value better is the cluster configuration. For our dataset we perform the silhouette analysis giving as a result the index value $0.145$ for $k = 8$.
\begin{figure}[h]
	%	\begin{minipage}[b]{0.8\textwidth}
	\centering
	\includegraphics[scale=0.50]{../Figures/silhouette-kmedoids_k8_seed0_lite}
	\caption{Silhouette Index for $k=8$.}
	\label{Fig:Silhouette-kMedoids}
	%	\end{minipage}
\end{figure}

% TODO Smooth Spline Fitting
An additional method to find the optimal value for $k$, based on the inflection point, consists in fitting the values of total sum of intra cluster distances using a cubic smooth spline. Splines are a smooth and flexible way of fitting Non linear Models and learning the Non linear interactions from the data. We are interested in finding the point where the curvature of the fitted model is the maximum, for our dataset the computed value is $k = 66$.(See Figure \ref{Fig:SmoothSpline-kMedoids})

\begin{figure}[h]
	%	\begin{minipage}[b]{0.8\textwidth}
	\centering
	\includegraphics[scale=0.5]{../Figures/SmoothSpline-kMedoids}
	\caption{Smooth Spline Fitting Method to find the optimal $k$ for the $k$--Medoids Approach.}
	\label{Fig:SmoothSpline-kMedoids}
	%	\end{minipage}
\end{figure}

We can observe that several methods to find an optimal value for $k$ give us different values. Figure \ref{Fig:OptimalkKMedoids} shows the resulting groups when applying two of these partitioning techniques; the marks indicate the representative points of each group. %TODO Argumentar que puede estar pasando
The following table is a summary of the optimal values for $k$ found in each method:
\begin{table}[h]
	\centering
	\tiny
	\begin{tabular}{|l|c|}
		\hline
		Method & Optimal $k$ \\ \hline
		Elbow  & 4	\\
		Silhouette & 8	\\
		Smooth Spline for SSE & 66\\ \hline
	\end{tabular}
	\caption{Methods to find the optimal value for $k$.}
	\label{Table:ValidationIndex}
\end{table}

\begin{figure}[htb]
	
	\centering
	\subfloat{%[DTW distance from the point $(0,0)$ to the other elements.]{\label{Fig:DTW-Distance0}
		\centering
		\includegraphics[width=0.45\linewidth]{../Figures/query-kmedoids_k8_seed0_lite__region-0-1-0-1}
	}
	%no space
	\hfill
	\subfloat{%[DTW distance from the center to the other elements.]{\label{Fig:DTW-DistanceCenter}
		\centering
		
		\includegraphics[width=0.45\linewidth]{../Figures/query-kmedoids_k66_seed0_lite__region-0-1-0-1}
	}
	
	\caption{Groups obtained with $k$--Medoids using $k=8$ (left) and $k=66$ (right).}
	\label{Fig:OptimalkKMedoids}
\end{figure}	

\subsubsection{Discussion on Domain Partitioning}
\label{Sec:DomainPartitioningDiscussion}

% Justification and Summarization
Spatial-temporal data are heterogeneous and autocorrelated, consequently the data are consistent and smoothly variable. The division of the domain into $k$ parts by a method that considers the similarity in the temporal evolution of its elements is indeed superior to a method that does not consider any similarity. Furthermore, there is more than one $k$ that, based on temporal similarity, expresses better different regions of the domain. So, we can consider different sizes $k$ for the domain partitioning, for example those that are targeted by the validation methods of $k$ in the $k$-- medoids algorithm. 

%TODO highlight nice results
%TODO hypothesis validated?

% DBSCAN
%De la misma forma, podriamos considerar otros metodos de clusterizacion donde usemos la medida de similaridad DTW, uno de ellos es DBSCAN que es un pocedimento basado en la densidad espacial de los datos. Utilizando dos parametros, rayo maximo para aglomerar puntos y numero minimo de elementos por grupo. Este algoritmo nos da como salida el numero de grupos encontrados en el dominio, pero la desventaja es que al ser un algoritmo que encuentra unicamente grupos convexos, si los elementos no son similares son considerados outliers. Los resultados obtenidos son los siguientes:
In addition to $k$--Medoids, we also explored DBSCAN, another clustering method that can also use DTW as the similarity measure. DBSCAN is a procedure to find `optimal' number of groups based on the spatial density of the data, and uses two parameters: the maximum ray to agglomerate points and the minimum number of elements per group. Its disadvantage is that being an algorithm that only finds convex groups, if the elements are not similar they are considered outliers. When DBSCAN was applied to our dataset, it would either find only two groups containing most of the data points, or few groups that failed to contain most of the points, because DBSCAN would mark other points as outliers. After this exploration of DBSCAN, further analysis with it was discontinued.

In the following section, we describe the process to generate predictive models on the representative elements and the experiments to evaluate its predictive quality.

\subsection{Forecast Error Analysis of Predictive Models on Representatives}
\label{Sec:AnalyzePredictorRepresentatives}

% Objective: Demonstrate that it is sufficient to use a models over a group representative for every group element.
In this section we perform extensive experiments to evaluate the predictive power of models trained on the representative elements of each group. 

% Predictor ARIMA
Following the Step 2 of our proposed methodology, (Section \ref{Sec:ModelRepresentatives}, we generate an ARIMA model for each representative obtained from the domain partitioning. We denote the subset of representative elements for different values of $k$ by $\mathcal{R} \subset \mathcal{D}$, and the generated models $g$ as per equation \ref{eq:ModelDefinition}, $g \in \mathcal{G}_{(\mathcal{R})}$. 
%TODO revisar
In this context, the model parameters $\mathbf{p}$ comprise both the ARIMA $(p, d, q)$ parameters, and the default hyper-parameters used in an implementation \texttt{auto.ARIMA} that finds a suitable $(p, d, q)$. When using \texttt{auto.ARIMA}, the route performs a grid search that selects the $(p, d, q)$ model parameters based on the minimization of the AIC (Akaike Information Criteria) \cite{Hyndman2008}.

%For each representative we generate a predictive model, in particular an ARIMA model that is trained using an implementation based on the \texttt{auto.ARIMA} routine which perform a grid search for the parameters $(p,d,q)$ and select them based on the minimization of the AIC (Akaike Information Criteria) \cite{Hyndman2008}. 

\subsubsection{Obtaining In-sample and Forecast Errors}

%TODO revisar
Figure \ref{Fig:Time-SeriesModel} shows the division of time units on the time-series in order to train and test an ARIMA model. Recalling the preparation of the dataset described in Section \ref{sec:DatasetPreparation}, we work with 365 values for each representative point, denoted by the series $s[0:364]$.

Let's consider a particular representative $r \in \mathcal{R}$ and its associated temperature series $s_r$. To evaluate an ARIMA model $g_{s_r}$ and its predictive power, the series is then split into three parts. The first part is used for training the model ($s^t_r[0:349]$ as an example in Figure \ref{Fig:Time-SeriesModel}), here we determine the tuple $(p_{s_r}, d_{s_r}, q_{s_r})$ that corresponds to the model $g_{s_r}$. Then, we take the next $t_p$ data points ($t_p = 8$ in our example) to create the validation series $s^v_t[349:357]$. The purpose of this subset is to obtain the in-sample error ${E^*}$ of the model: this is achieved by using the model $g_{s_r}$ to make a prediction of $t_p$ steps, and then calculating an associated error using one of the error expressions presented in Section \ref{Sec:ModelRepresentatives}.

To enable forecasting, the model is retrained using the same $(p, d, q)$ model parameters found with $s^t_r$, but now using a longer series $s[0:357]$. In order to evaluate the model's predictive power, we use it to forecast $t_f$ steps into the future ($t_f = 8$ in our example) that the model has not seen. But, since the subset $s[357:364]$ is actually available from the dataset, we are evaluate to calculate the forecast error, allowing us to perform forecast error analysis on different representatives from different domain partitioning schemes, as well as other predictive models used as baseline ($k$NN).

%TODO describe figure (each component) in text
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.35]{../Figures/ModelRegion_ModelTS}
	\caption{Dataset Split to Generate Time-Series Predictors.}
	\label{Fig:Time-SeriesModel}
\end{figure}

\subsubsection{Analyzing Forecast Errors}

%consider a predictive model generated over a set of representatives of the spatio--temporal domain, taking into consideration that some representatives can be more suitable than others for a particular element in the domain, in terms of the generalization error obtained when using the model of a representative to create a forecast for the time series in another element. 
% TODO no free lunch, etc
% Forecast Error vs. DTW distance intra-cluster
% The forecast values for models 
% Forecast Error of predictor over representative compare with the 
%TODO rephrase "almost impossible", e.g. "not feasible" or "cannot find a significant correlation..."
Given that the confidence intervals of the generalization error for the model built for the representative element is a varying value between the forecast error of the model composition listed, we need to analyze the intra cluster variability. When applying $k$--medoids to the domain, the number of elements on each cluster are different and it is almost impossible to find a correlation between the model composition forecast error and the DTW distance from the medoid.
% Compute the confidence intervals for representatives, explain the process and show at least two representatives
%DTW distance between representatives

% DTW distance and generalization error 
Analyze the generalization error for the predictor in each representative, considering that the 

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{../Figures/auto_arima_distance_error_whole_brazil_1y_1ppd_k8_seed1_cluster4_pdq2-1-1_sMAPE}
	\caption{Forecast Error vs. DTW distance in a group $k$--medoids with $k=8$}
	\label{Fig:DTWvsForecastError}
\end{figure}

\subsubsection{Predictive Quality of Model Composition}
\label{Sec:ModelComposition}

As explained in the methodology we are interested in evaluate the generalization error for a model composition, a model composition is formed by one or more predictors which will be used to perform a prediction on a domain region. Recalling from Section \ref{Sec:ModelRepresentatives}, the model composition of interest are:

\begin{itemize}%[noitemsep,nolistsep]
	\item Model Composition with Minimal Error: for each group, we find the time series for which its predictive model, when applied to all elements of its corresponding group, minimizes the accumulated forecast error. This can be considered the `best' choice for representatives that can only be found with an exhaustive approach.
	\item Model Composition with Minimal Local Error: for each time series in each group, we find its own predictor and calculate the corresponding forecast error. We can say that this would be the forecast error of a naive approach, because it generates as many predictors as there are time series in the region.
	\item Model Composition formed by the Medoids: For the time series representatives in each group, we find the corresponding predictors and calculate the forecast error in all elements of each group. This corresponds to the proposed methodology.
	\item Model Composition with Maximum Error: for each group, we find the time series for which its predictive model, when applied to all elements of its correspond
\end{itemize}

By comparing qualitatively the in-sample error for these model compositions we guarantee the validity of using the model for the representative as a good generalization of the group. At the same time we reduce the number of predictive models generated for a spatio--temporal domain. As a part of the off--line process we store the resultant predictive models, this will allow us to speed--up the task of compute a forecast for a time-series.

% Forecast Error in a group
In the previous step we consider maintain several domain partitioning schemes. Considering the values for $k = \{4, 8, 66\}$, we compute the forecast error of the model composition listed above, applied over its group generated in the partitioning technique. In the following tables we 

\begin{table}[h]
	\centering
	\tiny
	\begin{tabular}{|c|r|r|r|r|r|r|r|r|r|r|}
		\hline
		cid & size & $(p, d, q)$ & AIC & T. F. (seg) & T. T. (seg) & Each & Min. & Min. Local & Medoid & Max. \\
		\hline
		0 &  990 & (2, 1, 2) & -727.287 & 1.642 & 3601.959 & 0.333 & 0.305 & 0.379 & 0.331 & 0.826 \\
		1 &  990 & (0, 1, 2) & -655.858 & 1.665 & 3221.489 & 0.192 & 0.180 & 0.198 & 0.268 & 0.517 \\
		2 &  990 & (1, 1, 2) & -769.059 & 1.621 & 4063.693 & 0.384 & 0.460 & 0.888 & 0.587 & 1.608 \\
		3 &  990 & (2, 1, 1) & -896.605 & 1.610 & 3424.456 & 0.341 & 0.387 & 0.634 & 0.394 & 1.028 \\
		4 &  990 & (2, 1, 2) & -744.805 & 1.651 & 4305.636 & 0.782 & 0.571 & 1.182 & 0.769 & 3.058 \\
		5 &  990 & (1, 1, 2) & -688.749 & 1.713 & 3818.287 & 0.470 & 0.489 & 0.580 & 0.609 & 2.062 \\
		6 & 1080 & (0, 1, 0) & -659.159 & 1.744 & 4674.208 & 1.190 & 0.564 & 0.862 & 2.454 & 2.935 \\
		7 & 1080 & (2, 1, 1) & -681.813 & 1.795 & 4847.003 & 0.546 & 0.463 & 0.563 & 0.583 & 1.188 \\ \hline
	\end{tabular}
	\caption{Model Composition Forecast Error for Regular Partitioning with $k=10$.}
	\label{Table:ForecastErrorRegulark10}
\end{table}

In the table \ref{Table:ForecastErrorRegulark10}, the parameters $(p, d, q)$ and  AIC corresponds to the ARIMA predictor for the time-series representative. T.F is the elapsed forecast time for 8 time units and T.T is the total time used for training and forecast. 

For the $k$--medoids partitioning we compute the same values:
\begin{table}[h]
	\centering
	\tiny
	\begin{tabular}{|c|r|r|r|r|r|r|r|r|r|r|}
		\hline
		cid & size & $(p, d, q)$ & AIC & T. F. (seg) & T. T. (seg) & Each & Min. & Min. Local & Medoid & Max. \\
		\hline
		0 &  574 & (0, 1, 2) & -782.717 & 1.069	&  2041.469	& 0.170	& 0.161	& 0.289	& 0.185	& 0.438	 \\
		1 &  817 & (2, 1, 2) & -814.564 & 1.299	&  3447.608	& 0.689	& 0.460	& 0.531	& 0.926	& 1.566	 \\
		2 &  542 & (1, 1, 2) & -782.089 & 0.880	&  2011.441	& 0.581	& 0.514	& 0.577	& 0.678	& 1.420	 \\
		3 &  755 & (1, 1, 1) & -767.516 & 1.238	&  2685.912	& 0.413	& 0.289	& 0.364	& 0.492	& 0.878	 \\
		4 & 3479 & (1, 1, 2) & -759.429 & 5.727	& 14542.318	& 0.785	& 0.475	& 0.548	& 0.838	& 1.983	 \\
		5 &  803 & (1, 1, 1) & -737.956 & 1.375	&  3231.718	& 0.407	& 0.294	& 0.378	& 0.437	& 1.194	 \\
		6 &  625 & (3, 1, 1) & -650.722 & 0.957	&  1930.740	& 0.157	& 0.168	& 0.175	& 0.203	& 0.478	 \\
		7 &  505 & (3, 1, 1) & -660.001 & 0.853	&  1811.335	& 0.388	& 0.375	& 0.422	& 0.551	& 1.015	 \\ \hline	
	\end{tabular}
	\caption{Model Composition In-Sample Error for Partitioning Technique  $k$--Medoids with $k=8$ and \texttt{seed 0}.}
	\label{Table:ForecastErrorkMedoidsk10}
\end{table}

% Analyze the prediction error for solvers in a region 10x10 when varying k

Experimentally we can see that when considering the prediction of the medoid as the prediction for the points of its group, there is a balance in the final prediction value in the analyzed region. This ensures that we can use the medoids and their respective predictive models to predict the time evolution of the entire domain.

%Experimentalmente podemos ver que al considerar la prediccion del medoide como la prediccion para los puntos de su grupo, existe un balance en el valor final de prediccion en la region analizada. Esto garantiza que podamos usar los medoides y sus respectivos modelos predictivos para predecir la evolucion temporal del dominio completo.

For each considered $k$, it is possible to observe that the predictions made by the models in the medoids are lower, this is due to the fact that the points in each group are increasingly similar. There are exceptions since the models considered are built on historical data from a point in the region, and the variations may be the product of an unexpected change in the behavior of the temperature at that point.

%Para cada $k$ considerado es posible observar que las predicciones realizadas por los modelos en los medoides son mas bajos, esto es debido a que los puntos en cada grupo son cada vez mas similares. Existen excepciones dado que los modelos considerados estan construidos sobre datos historicos de un punto en la region, y las variaciones pueden ser producto de un cambio inesperado en el comportamiento de la temperatura en ese punto. 

The advantage of an off-line process is the ability to maintain various domain division schemes (consequently various predictive models), in such a way that their representatives are shown as generalizations of various regions in the domain. The next step is to develop a mechanism that considers the composition of models for the prediction of temperature in a region of interest, considering the total set of representatives generated by more than one $ k $. 

%La ventaja de un proceso off-line es la capacidad de mantener varios esquemas de division del dominio (en consecuencia varios modelos predictivos), de tal forma que sus representantes se muestren como generalizaciones de diversas regiones en el dominio. El siguiente paso consiste en desarrollar un mecanismo que considere la composicion de modelos para la prediccion de temperatura en una region de interes, considerando el conjunto total de representantes generados por mas de un $k$.

\subsection{Building a Classifier}
\label{Sec:ExperimentsTrainingClassifier}

In the Methodology, Sections \ref{Sec:Classifier} and \ref{Sec:TrainingClassifier}, we describe the implementation of a Neural Network Model for the Time-Series Multiclass Classification task. First we extract the dataset, each sample is formed by a time-series $\mathcal{S}$, and a label $y$. The label is a tuple that represents domain partitioning scheme and the group in a partitioning technique (Section \ref{Sec:KnowledgExtraction}). Considering maintaining domain partitioning schemes for $k = \{8, 66, 132\}$, and its corresponding predictive models on representatives. The following table represent the dataset extracted (described on Table \ref{Tab:TSClassificationDataset}) to generate a NN Model:

\begin{table}[h]
	\centering
	\small
	\begin{tabular}[h]{|c|c|c|c|c|c|c|c|c|c|}
		\hline
		& s0    & s1    & s2    & s3    & s4    &	s5    & $\ldots$ & s364  &   label \\ \hline
		0 & 0.484 & 0.563 & 0.443 & 0.386 & 0.326 &	0.391 & $\ldots$ & 0.359 &   8-3 \\
		1 &	0.482 &	0.508 &	0.520 &	0.477 &	0.354 &	0.444 & $\ldots$ & 0.361 & 132-69 \\
		2 &	0.639 & 0.618 &	0.549 &	0.609 &	0.433 & 0.476 & $\ldots$ & 0.421 &	66-26 \\ 
		$\vdots$  & &     &       &       &       &       &          &       & $\vdots$ \\ \hline
	\end{tabular}
	\caption{Dataset with 3000 instances for Time Series Classification.}
	\label{Table:DatasetTSC}
\end{table}

% Architectures considered
We consider two independent architectures, Long-Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) and third hybrid CNN-LSTM. The architecture hyperparameters and optimization parameters are listed in tables \ref{Table:HyperparametersNN} and \ref{Table:OptimizationNN}. In the first table we can see the number and type of layers and their respective activation function, Rectified Linear Unit (ReLU), for the CNN we consider the MaxPooling option and we regularization dropout (0.4).

All models were optimized using a variant of SGD such as Adam (\cite{Kingma2015}), and the batch size that defines the number of samples to work through before updating the internal model parameters. For the fourth model we consider a lower learning rate which require more training epochs given the smaller changes made to the weights each update \cite{}.

\begin{table}[h]
	\centering
	\tiny
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{Methods} & \multicolumn{7}{c|}{Architecture} \\
		\cline{2-8}
		& \#layers & \#conv & \#lstm & norm & pooling & activate & regularize \\
		\cline{2-8}
		\hline
		LSTM & 2 & 0 & 2 & none & none & ReLU & dropout \\
		\hline
		1DCNN & 2 & 2 & 0 & none & max & ReLU & dropout \\
		\hline
		1DCNN--LSTM & 4 & 2 & 2 & batch & max & ReLU & dropout  \\
		\hline
		% etc. ...
	\end{tabular}
	\caption{Architecture’s hyperparameters.}
	\label{Table:HyperparametersNN}
\end{table}

\begin{table}[h]
	\centering
	\tiny
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{Methods} & \multicolumn{6}{c|}{Optimization} \\
		\cline{2-7}
		& alg. & valid. & loss & epochs & batch & learn \\
		\cline{2-7}
		\hline
		LSTM & Adam & Split 20\% & Cross Entropy & 150 & 64 & 0.001 \\
		\hline
		1DCNN & Adam & Split 20\% & Cross Entropy & 100 & 64 & 0.001 \\
		\hline
		1DCNN-LSTM & Adam & Split 20\% & Cross Entropy & 200 & 128 & 0.001 \\
		\hline
		1DCNN-LSTM & Adam & Split 20\% & Cross Entropy & 300 & 128 & 0.0001 \\
		\hline
	\end{tabular}
	\caption{Optimization’s hyperparameters.}
	\label{Table:OptimizationNN}
\end{table}

% Dataset transformation
% Sliding Window
For both architectures we need to reshape the dataset which originally is \texttt{(3000, 366)}, representing number of samples and dataset (365 instances of an univariate time series and the label). A way to look at it is, interpret a time-series as 2 dimensional data, where the first dimension is time-steps and other is the values of the  temperature in 1 axe. For the model \texttt{input\_shape = (None, 15, 1)}, represents 15 time-steps with 1 data points in each time step. These 1 data points are the temperature in 1 axe. The transformation of the dataset is the following:

\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={Input Shape Sizes for CNN1D-LSTM Model}}
\lstset{label={lst:shape_dataset}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
# First Shape:
num_features = 1 (Temperature)
n_length =  15
input_shape = (None, n_length, num_features)
\end{lstlisting}

We implement the architectures in \texttt{keras} (\cite{Chollet2015}), 


\lstset{language=Python}
\lstset{frame=lines}
\lstset{caption={CNN1D-LSMT Model for Multiclass Classification.}}
\lstset{label={lst:code_direct}}
\lstset{basicstyle=\footnotesize}
\begin{lstlisting}
# CNN1D-LSTM Model
model = Sequential()
model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), 
			input_shape=(None, n_length, num_features)))
,model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))
model.add(TimeDistributed(Dropout(0.4)))
model.add(TimeDistributed(MaxPooling1D(1)))
model.add(TimeDistributed(Flatten()))
model.add(BatchNormalization())
model.add(LSTM(1024))
model.add(Dropout(0.4))
model.add(BatchNormalization())
model.add(Dense(1024, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(number_labels, activation='softmax'))

optimizer = keras.optimizers.Adam(lr=0.0001)

model.compile(optimizer=optimizer, loss='categorical_crossentropy', 
		metrics=['categorical_accuracy'])

print(model.summary())
\end{lstlisting}

% Results: Accuracy and Validation Loss for each model

The resultant accuracy of the first two models was very low, which means that the model was not able to predict the label for a new time-series. As a result both models learnt only one class.

\begin{table}[h]
	\centering
	\tiny
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Model        & Layers & Batch Size & Learning Rate & Accuracy & Validation Loss \\ \hline
		LSTM         & 2      &  64        & 0.001         &  2.610   & 4.630           \\
		CNN1D        & 2      & 128        & 0.001         & 17.600   & 3.380           \\
		CNN1D--LSTM  & 2-2    & 128        & 0.001         & 57.740   & 2.673           \\ 		
		CNN1D--LSTM  & 2-2    & 128        & 0.0001		   & 64.759   & 1.865           \\ \hline		
	\end{tabular}
\caption{NN Models Training Metrics for TSC.}
\label{Table:DLModels}
\end{table}


\subsubsection{Discussion}

In the third model, a CNN1D-LSTM, we obtain higher accuracy. Performing a set of parameters tuning (considering several values for the learning rate and batch size). The variation of these parameters affects the training time and how fast we achieve convergence in the validation loss function, the following table shows the results obtained:

The key element in these experiments is to find a sweet spot between the window size, learning rate and the batch size. By lowering the learning rate we take smaller steps in order to find the best possible weights for the model and considering a greater batch size we speed-up the training of the model and reduce the noise added to the dataset, which is desirable for time-series data.

% Justify 
Having a sufficiently large time delay window is important for a time series predictor - if the window is too small then the attractor of the system is being projected onto a space of insufficient dimension, in which proximity is not a reliable guide to actual proximity on the original attractor. Thus, two similar time delay vectors y1 and y2, might represent points in the state space of the system which are actually quite far apart.  Moreover, a window of too large a size may also produce problems: since all necessary information is populated in a subset of the window, the remaining fields will represent noise or contamination.  Various heuristics can be used to estimate the embedding dimension, and here we use the false nearest neighbor method and the singular-value analysis \cite{Fawaz2019}.

%Time-Series Classification is a difficult problem and recently the use of Deep Learning techniques ae becoming the default to solve this problem When dealing with univariate time-series 

\subsection{Spatio--Temporal Predictive Query Processing}

In this Section we show the results and discussion for the on--line procedure implemented to execute a query in order to obtain a prediction (Section \ref{Sec:SpatioTemporalQueryProcessing}). A spatio-temporal predictive query is defined in Section \ref{Sec:ProblemFormalization}  as the tuple:

\begin{equation*}
Q = \langle R, t_{p}, t_{f}, Q_{m} \rangle,
\end{equation*}
where:
\begin{itemize}[noitemsep,nolistsep]	
	\item $R$: represents the size/shape/type of interest region,
	\item $t_{p}$: $\{s_{t-t_p}, s_{t-t_{p}+1}\ldots, s_{t}\}$ number of steps used for  prediction,
	\item $t_{f}$: $\{s_{t+1}, \ldots, s_{t+t_f}\}$ number of steps to predict ($n\geq 1$),
	\item $Q_{m}$: represents users qualitative measurements to evaluate the predictive output.
	%\item $Y_{Q}$: prediction output.
\end{itemize}

The data processing step consist in extract elements from the domain corresponding to the query region with the $t_p$ instances, we denote this as $[R \times t_{p}]$. Next to compute the forecast value for the query, we use one of the following approaches to perform the model composition:

\begin{itemize}
	\item Composition of Point Predictive Models (ARIMA and $k$NN).
	\item Composition of Representative Predictive Models.
	\item Composition of Classifier for Predictive Models.	
\end{itemize}

In the following section we describe the results of the experiments considering queries with different sizes and over different regions of the domain.

\subsubsection{Evaluating Spatio--Temporal Predictive Queries}
\label{Sec:ExperimentsQueries}

We define a spatio--temporal predictive query as the tuple $Q = \langle R, t_{p}, t_{f}, Q_{m} \rangle$, for the experiments performed here we have considered $t_{p} = 8$ and $t_{f} = 8$. For the model composition we consider the following type of models:

\begin{enumerate}
	\item Representatives from $k$--Medoids Domain Partitioning.
	\item Representatives from Regular Domain Partitioning.
	\item Selected by the Classifier for several domain partitioning schemes.
\end{enumerate}

We compare the results of the previous predictions with a point model composition strategy, which are computationally more expensive, the considered forecast methods are: 
\begin{itemize}
	\item $k$ Nearest Neighbor ($k$NN).
	\item AutoRegressive Integrated Moving Average (ARIMA).
	%	\item The DJEnsemble, a spatio-temporal approach.
\end{itemize}

To analyze the variation in the value of the prediction model in each composition composed consider a mesh size $10\times 10$ squares over the entire domain. Consider spatio--temporal predictive queries where the region of interest corresponds to each square (Figure \ref{Fig:Query_10x10_whole_real_brazil}). We also consider queries with a region of interest of size $20\times 20$, $30 \times 30$, $15 \times 20$ and $30 \times 15$. 

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.75]{../Figures/query_10x10_whole_real_brazil}
	\caption{Mesh to consider Spatio--Temporal Predictive Queries with Region of size $10 \times 10$.}
	\label{Fig:Query_10x10_whole_real_brazil}
\end{figure}

The results for $81$ spatio--temporal predictive queries, product of considering the mesh of squares ($10 \times 10$) over the domain, are described in the following table. For each domain partitioning scheme with $k = \left\{8, 66, 132 \right\}$, we present a descriptive statistics summary for the prediction query value computed. We can see the variation of mean and its respective standard deviation for the prediction value when considering the model composition formed by the representative models in each domain partitioning technique: $k$--Medoids and Regular.

\begin{table}[h!]
	\centering
	\tiny
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{Dom. Partitioning Technique} & \multicolumn{3}{c|}{$k$--Medoids} & \multicolumn{3}{c|}{Regular} \\
		\cline{2-7}
		& $k = 8$. & $k = 66$ & $k = 132$ & $k = 8$ & $k = 66$ & $k = 132$ \\
		\cline{2-7}
		\hline
		Forecast Error ($t_{f}=8$) & $0.48 \pm 0.59$ & $0.47 \pm 0.86$ & $0.39 \pm 0.62$ & $1.05 \pm 2.07$ & $1.17 \pm 2.59$ & $0.55 \pm 0.68$	 \\
%		Forecast Error ($t_{f}=8$) & $0.48 \pm 0.59$ & $0.47 \pm 0.86$ & $0.39 \pm 0.62$ & $1.05 \pm 2.07$ & $0.45 \pm 0.69$ & $0.44 \pm 0.66$	 \\
		\hline
	\end{tabular}
	\caption{Forecast Error Summary.}
	\label{Table:Query10x10_kMedoids_Regular_StatSummary}
\end{table}

According to Table \ref{Table:Query10x10_kMedoids_Regular_StatSummary}, the model composition formed by representative models of the $k$--Medoids representatives predict better than the regular approach. Analisis experimental que no es tan riguroso, pues solo muestra una variacion del error para regiones 
The following figures are heat maps of the computed forecast errors for the considered spatio--temporal predictive queries:

\begin{figure}[ht]
	\subfloat[$k$--Medoids.]{
		\begin{minipage}[c][1\width]{
				0.5\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{../Figures/query_10x10_kmedoids_k8-300dpi}
	\end{minipage}}
	\hfill 	
	\subfloat[Regular.]{
		\begin{minipage}[c][1\width]{
				0.5\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{../Figures/query_10x10_regular_k8-300dpi}
	\end{minipage}}
	\caption{Model Composition formed by Models Representatives in a Domain Partitioning Technique ($k=8$)}
	\label{Fig:query_10x10_kmedoids_regular_k8}
\end{figure}

\begin{figure}[h!]
	\subfloat[$k$--Medoids.]{
		\begin{minipage}[c][1\width]{
				0.5\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{../Figures/query_10x10_kmedoids_k66-300dpi}
	\end{minipage}}
	\hfill 	
	\subfloat[Regular.]{
		\begin{minipage}[c][1\width]{
				0.5\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{../Figures/query_10x10_regular_k66_b-300dpi}
	\end{minipage}}
	\caption{Model Composition formed by Models Representatives in a Domain Partitioning Technique ($k=66$).}
	\label{Fig:query_10x10_kmedoids_regular_k66}
\end{figure}

\begin{figure}[h!]
	\subfloat[$k$--Medoids.]{
		\begin{minipage}[c][1\width]{
				0.5\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{../Figures/query_10x10_kmedoids_k132-300dpi}
	\end{minipage}}
	\hfill 	
	\subfloat[Regular.]{
		\begin{minipage}[c][1\width]{
				0.5\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{../Figures/query_10x10_regular_k132_b-300dpi}
	\end{minipage}}
	\caption{Model Composition formed by Models Representatives in a Domain Partitioning Technique ($k=132$.)}
	\label{Fig:query_10x10_kmedoids_regular_k132}
\end{figure}

%Los experimentos realizados en las Secciones \ref{Sec:AnalyzeDomainPartitioning} y \ref{Sec:AnalyzePredictorRepresentatives} permitieron evaluar la calidad de agrupamiento en cada tecnica, asi como la calidad predictiva de modelos generados en los puntos representantes respectivamente. Con estos resultados evaluamos la calidad predictiva de los modelos generados en los puntos representantes cuando son usados en la composicion de modelos para la ejecucion de una consulta predictiva espacio-temporal. Observamos que es preferible usar una estrategia de comparacion de elementos (en la dimension temporal) y agrupamiento de los mas similares entre si, para lograr generalizar un grupo de puntos por medio de un punto. Este representante generaliza la evolucion temporal y su modelo predictivo esta dentro de un limite de prediccion para valores futuros.

The experiments carried out in Sections \ref{Sec:AnalyzeDomainPartitioning} and \ref{Sec:AnalyzePredictorRepresentatives} enable the evaluation and analysis of the grouping quality for each partitioning technique, as well as the predictive quality of the models generated in the representative points, respectively. With these results we evaluate the predictive quality of the models generated in the representative points when they are used in the composition of models for the execution of a spatio-temporal predictive query. 
%We observe that it is preferable to use a grouping strategy based on the comparison of elements (in the temporal dimension) and grouping of the most similar to each other, in order to generalize a group of points by means of a point. 
This representative generalizes the temporal evolution and its predictive model is within a prediction limit for future values.

%Experimentalmente observamos que para diferentes valores de $k$, la composicion de modelos tambien presenta variaciones en los valores de prediccion, producto de las propiedades en datos espacio--temporales. Cuando consideramos la presencia de varios esquemas de particion de dominio, usamos el clasificador de modelos para series temporales, que corresponde a la propuesta de este trabajo.
Experimentally we observe that for different values of $k$, the composition of models also presents variations in the prediction values, this is due to the properties of spatio--temporal data. When we consider the presence of several domain partitioning schemes, we use the model classifier for time series, which corresponds to our proposal.

Next we show the results of model composition using the classifier of predictive models for univariate time--series, and also the results when the model composition is the point model generation.

\begin{table}[h!]
	\centering
	\tiny
	\begin{tabular}{|c|c|c|}
		\hline
		Model Composition & Model Classifier & Model Each Point (ARIMA)\\
		\hline
		Forecast Error ($t_{f}=8$) & $1.07 (s = 1.59)$ & $0.38 (s = 0.61)$ \\
		\hline
	\end{tabular}
	\caption{Forecast Error Summary.}
	\label{Table:Query10x10_Classfier_Point_Each_StatSummary}
\end{table}

In the following Figure we show the heat map for the prediction values computed during the query execution.

\begin{figure}[h!]
	\subfloat[$k$--Medoids.]{
		\begin{minipage}[c][1\width]{
				0.5\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{../Figures/query_10x10_kmedoids_k132-300dpi}
	\end{minipage}}
	\hfill 	
	\subfloat[Regular.]{
		\begin{minipage}[c][1\width]{
				0.5\textwidth}
			\centering
			\includegraphics[width=1\textwidth]{../Figures/query_10x10_regular_k132_b-300dpi}
	\end{minipage}}
	\caption{Model Composition by Models Representatives in a Domain Partitioning Technique ($k=132$.)}
	\label{Fig:query_10x10_classifier_each}
\end{figure}

Next we consider different sizes for the region of interest, and the next table shows the region size and the total number of queries considered to compute its predictions:

\begin{table}[h]	\centering
	\tiny
	\begin{tabular}{|c|c|r|r|r|r|r|r|r|r|r|}
		\hline
		\multirow{3}{*}{Size} & \multirow{3}{*}{Query Region} & \multicolumn{9}{c|}{Model Composition} \\ 
		\cline{3-11}
		& & \multicolumn{3}{c|}{$k$--Medoids} & \multicolumn{3}{c|}{Regular} & \multirow{2}{*}{Classifier} & \multirow{2}{*}{$k$NN} & \multirow{2}{*}{ARIMA} \\ 
		\cline{3-8}
		& & $k =8$ & $k = 66$ & $k = 132$ & $k=8$  & $k = 66$ & $k = 132$ &  & & \\ \hline % & DJEnsemble \\ \hline
		$20 \times 20$ & $[ 0, 20] \times [ 0, 20]$ & 0.089 & 0.174 & 0.160 & 0.094 & 0.169 & 0.205 & 0.190 & 0.209 & 0.158  \\ % &  935.748\\ %30.590 \\
		$20 \times 20$ & $[20, 40] \times [35, 55]$ & 0.335 & 0.199 & 0.230 & 0.395 & 0.234 & 0.225 & 0.330 & 0.258 & 0.203 \\ % &  993.826\\ %31.525 \\
		$20 \times 20$ & $[50, 70] \times [60, 80]$ & 0.584 & 0.203 & 0.188 & 0.475 & 0.165 & 0.186 & 0.274 & 0.145 & 0.170 \\ % & 6572.021\\ %81.068 \\
		$20 \times 20$ & $[15, 35] \times [65, 85]$ & 0.063 & 0.045 & 0.038 & 0.106 & 0.074 & 0.057 & 0.093 & 0.067 & 0.034 \\ % & 4641.424\\%68.128 \\
		$30 \times 30$ & $[20, 50] \times [50, 80]$ & 0.203 & 0.147 & 0.135 & 0.170 & 0.180 & 0.158 & 0.202 & 0.210 & 0.122 \\ % & 1410.378\\%37.555 \\
		$30 \times 30$ & $[15, 45] \times [20, 50]$ & 0.262 & 0.155 & 0.168 & 0.367 & 0.199 & 0.184 & 0.281 & 0.198 & 0.156 \\ %  & 1205.687\\%34.723 \\
		$15 \times 20$ & $[40, 55] \times [20, 40]$ & 0.707 & 0.530 & 0.541 & 0.368 & 0.682 & 0.581 & 0.618 & 0.707 & 0.483 \\ % &  905.829\\%30.097 \\
		$15 \times 20$ & $[65, 80] \times [50, 70]$ & 0.470 & 0.302 & 0.308 & 0.353 & 0.266 & 0.300 & 0.343 & 0.190 & 0.248\\ %& 6382.732\\%79.892 \\
		$30 \times 15$ & $[30, 60] \times [ 5, 20]$ & 0.353 & 0.205 & 0.147 & 0.273 & 0.318 & 0.293 & 0.391 & 0.208 & 0.137 \\ % &  950.797\\%30.835 \\
		$30 \times 15$ & $[10, 40] \times [55, 70]$ & 0.139 & 0.111 & 0.098 & 0.127 & 0.158 & 0.115 & 0.135 & 0.226 & 0.095 \\ %& 4546.940\\ \hline% 67.431 \\ 
		\hline
	\end{tabular}
	\caption{MSE Forecast Error for Spatio-Temporal Queries in the domain $\mathcal{D}$.}
	\label{Table:MSEForecasError}
\end{table}
%
%
%\begin{table}[h]	\centering
%	\tiny
%	\begin{tabular}{|c|c|r|r|r|r|r|r|}
%		\hline
%		Query Size     & Query Region               & $k=8$  & $k = 66$  & $k = 132$ & CNN1D-LSTM & $kNN$     & ARIMA \\ \hline % & DJEnsemble \\ \hline
%		$20 \times 20$ & $[ 0, 20] \times [ 0, 20]$ &   2.193 &   6.864   &   4.529  &   6.391        &   8.276 &   \\ % &  935.748\\ %30.590 \\
%		$20 \times 20$ & $[20, 40] \times [35, 55]$ &  21.633 &  17.726   &  13.227  &  35.679        &  21.206 &  \\ % &  993.826\\ %31.525 \\
%		$20 \times 20$ & $[50, 70] \times [60, 80]$ &  31.521 &  17.116   &  14.880  &  29.974        &  15.028 & \\ % & 6572.021\\ %81.068 \\
%		$20 \times 20$ & $[15, 35] \times [65, 85]$ &   1.359 &   0.891   &   0.677  &   4.609        &   2.429 &  \\ % & 4641.424\\%68.128 \\
%		$30 \times 30$ & $[20, 50] \times [50, 80]$ &   9.663 &   7.159   &   6.681  &  14.220        &  11.494 & \\ % & 1410.378\\%37.555 \\
%		$30 \times 30$ & $[15, 45] \times [20, 50]$ &  14.345 &   8.971   &   7.394  &  28.748        &  13.333 & \\ %  & 1205.687\\%34.723 \\
%		$15 \times 20$ & $[40, 55] \times [20, 40]$ & 118.048 &  54.065   &  67.697  &  69.305        & 172.645 & \\ % &  905.829\\%30.097 \\
%		$15 \times 20$ & $[65, 80] \times [50, 70]$ &  26.438 &  17.143   &  15.875  &  25.243        &  30.781 & \\ %& 6382.732\\%79.892 \\
%		$30 \times 15$ & $[30, 60] \times [ 5, 20]$ &  36.496 &  11.322   &   9.305  &  60.515        &  20.782 & \\ % &  950.797\\%30.835 \\
%		$30 \times 15$ & $[10, 40] \times [55, 70]$ &   5.514 &   3.280   &   3.355  &   7.650        &   8.225 & \\ %& 4546.940\\ \hline% 67.431 \\ 
%		\hline
%	\end{tabular}
%	\caption{MSE Forecast Error for Spatio-Temporal Queries in the domain $\mathcal{D}$.}
%	\label{Table:MSEForecasError}
%\end{table}
		
%Query Sizes: 10 $\times$ 10
%Covering all the domain
%Query size 20 x 20
%Uno
%Lat : 0 - 20 
%Long: 0 - 20
%Dos
%Lat : 20 - 40
%Lon : 35 - 55
%Tres (Cobre RJ)
%Lat : 50 - 70
%Lon : 60 - 80
%Cuatro
%Lat : 15 - 35
%Lon : 65 - 85
%Query Size 30 x 30
%Cinco
%Lat : 20 - 50
%Lon : 50 - 80
%Seis
%Lat : 15 - 45
%Lon : 20 - 50
%Query Size 15 x 20
%Siete
%Lat : 40 - 55
%Lon : 20 - 40
%Ocho
%Lat : 65 - 80
%Lon : 50 - 70 
%
%Query Size 30 x 15
%Nueve 
%Lat : 30 - 60
%Lon : 5 - 20 
%
%Diez
%Lat : 10 - 40
%Lon : 55 - 70
%
The latitude and longitude for the queries are:

%Query size 20 x 20
%Uno
%Lat : (7.5) - (-2.5) 
%Long: (285.5) - (295.5)
%
%Dos
%Lat : (7.5) - (-12.5)
%Lon : (285.5) - (295.5)
%
%Tres (Cobre RJ)
%Lat : (-17.5) - (-27.5)
%Lon : (315.5) - (325.5)
%
%Cuatro
%Lat : (0) - (-10)
%Lon : 318 - 328
%
%Query Size 30 x 30
%Cinco
%Lat : (-2.5) - (-17.5)
%Lon : (310.5) - (325.5)
%
%Seis
%Lat : (0) - (-15)
%Lon : (295.5) - (310.5)
%
%Query Size 15 x 20
%Siete
%Lat : (-12.5) - (-20)
%Lon : (295.5) - (305.5)
%
%Ocho
%Lat : (-25) - (-32.5)
%Lon : (316.5) - (326.5) 
%
%Query Size 30 x 15
%Nueve 
%Lat : (-7.5) - (-22.5)
%Lon : (288) - (295.5) 
%
%Diez
%Lat : (2.5) - (-12.5)
%Lon : (319) - (326.5)

%In order to analyze the relationship that exists between the forecast error and the number of clusters in the two partition approaches, we scan the number of $k$ groups, thus generating different domain partitions. Then, we evaluated the resulting prediction error, for a region of interest $R$. The collected data are shown in Table \ref{Table:ErrorQuery}, in which the number of clusters at the intersection of the region (\# Groups) with the domain is shown, as well as the error of resulting forecast for region $R$.
%Com o objetivo de analisar a relação que existe entre o erro de previsão e o n\'umero de \textit{clusters} nas duas abordagens de partição, realizamos uma varredura do n\'umero de grupos $k$, gerando assim diferentes parti\c c\~oes do dom\'inio. Depois, avaliamos o erro de predi\c c\~ao resultante, para uma regi\~ao de interesse $R$. Os dados coletados s\~ao mostrados na tabela \ref{Table:ErrorQuery}, na qual \'e mostrada a quantidade de \textit{clusters} na interseção da região (\#Groups) com o domínio, bem como o erro de previsão resultante para a região $R$.
%\begin{table}
%	\centering
%	\tiny
%	\begin{tabular}{|c|c|c|c|c|}
%		\hline
%		& \multicolumn{2}{c|}{\textbf{Regular}} & 
%		\multicolumn{2}{c|}{\textbf{$k$--Medoids}} \\
%		\hline
%		$k$ & $\#$ Groups & sMAPE($R$)& $\#$ Groups & sMAPE($R$) \\
%		\hline
%		6	&	2	&	1.7949	&	6	&	0.9064	\\ \hline
%		8	&	4	&	1.8813	&	7	&	0.3887	\\ \hline
%		10	&	4	&	2.5577	&	7	&	0.3470	\\ \hline
%		12	&	2	&	1.9259	&	8	&	0.3575	\\ \hline
%		14	&	6	&	3.3218	&	11	&	0.3157	\\ \hline
%		16	&	4	&	1.6646	&	12	&	0.3404	\\ \hline
%		18	&	2	&	1.8541	&	11	&	0.2701	\\ \hline
%		20	&	4	&	1.9673	&	11	&	0.2623	\\ \hline
%		22	&	8	&	2.2890	&	12	&	0.2592	\\ \hline
%		24	&	4	&	2.3108	&	13	&	0.2550	\\ \hline
%		26	&	10	&	2.0771	&	13	&	0.2646	\\ \hline
%		28	&	6	&	2.2680	&	12	&	0.2622	\\ \hline
%		30	&	6	&	1.7552	&	13	&	0.2464	\\ \hline
%		32	&	6	&	2.0677	&	13	&	0.2384	\\ \hline
%		34	&	12	&	2.5048	&	14	&	0.2323	\\ \hline
%		36	&	4	&	1.7819	&	15	&	0.2185	\\ \hline
%		38	&	12	&	2.5048	&	15	&	0.2150	\\ \hline
%		40	&	9	&	1.6037	&	18	&	0.2134	\\ \hline
%		42	&	6	&	1.7679	&	18	&	0.2370	\\ \hline
%		44	&	8	&	1.7978	&	19	&	0.2113	\\ \hline
%		46	&	16	&	2.1264	&	18	&	0.2042	\\ \hline
%		48	&	6	&	1.6131	&	19	&	0.2334	\\ \hline
%		50	&	9	&	1.4307	&	20	&	0.2193	\\ 
%		\hline
%	\end{tabular}
%	\caption{Forecast Error for Spatio--Temporal Predictive Query with region size $10 \times 10$.}
%	\label{Table:ErrorQuery}
%\end{table}
%
%
%In the figure \ref{Fig:Forecast_Error_Query} the comparison of the prediction error between the two domain partitioning approaches is represented, with the same data as in Table \ref{Table:ErrorQuery}. It is concluded from the results that the domain partitioning approach by $k$--medoids consistently presents a predictive quality higher than regular partitioning. It is also observed that the prediction error begins to decrease as the value of $k$ increases for the $k$--medoid partitioning, but reaches a plateau.
%Na Figura  \'e representada a comparação do erro de previsão entre as duas abordagens de particionamento do domínio, com os mesmos dados da Tabela . Conclui-se dos resultados que a abordagem de caracterização do domínio por $k$--medóides apresenta, de maneira consistente, uma qualidade preditiva superior em comparação ao particionamento regular. Tamb\'em \'e observado que o erro de predi\c c\~ao come\c ca a diminuir conforme aumenta o valor de $k$ para o particionamento $k$--med\'oides, mas atinge um plat\^o.

%\begin{center}
%	\begin{figure}[h!]
%		%\begin{minipage}[b]{0.8\textwidth}
%		\centering
%		%			\vskip-20pt
%		\includegraphics[scale=0.40]{fig/Forecast_Error_sMAPE_Query_Region_35-55-0-30}
%		\caption{$k$--Medoids and Regular Forecast Error for $R$.}
%		\label{Fig:Forecast_Error_Query}
%		%\end{minipage}
%	\end{figure}
%\end{center}	

\section{Results and Discussion} 

The first part of this Chapter we describe the SPTA-TSA, an application developed to implement and analyze the proposed methodology. Next, we describe the outline of experiments and analysis performed to validate our hypothesis and decisions. 



\section{Final Comments}
