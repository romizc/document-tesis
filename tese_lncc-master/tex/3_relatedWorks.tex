\chapter{Related Works}
\label{chapter_Related_Works}

Spatio--temporal phenomena are present in almost every field of research, such as climate science \cite{Faghmous2014, Guevara2020}, geographic information systems (GIS) \cite{ElGeresy2002, Shekhar2015}, epidemiology \cite{Malchow2007, Cabrera2019}, among others. With the advancement of technology to acquire data for these phenomena, it is possible to store massive amounts of spatio--temporal data \cite{Atluri2018}. In this work, we are interested in the representation of a given phenomenon using univariate time series, such as temperature \cite{Muhammet2012, Romilly2005}, energy consumption \cite{Abdelaal2008}, or econometrics \cite{Moral2003}.

Analyzing and studying space-time phenomena through data is a complex task \cite{Rao2008}, and the case of univariate time series is not the exception. There are several tools available, depending on the type of analysis and information to be extracted. Some of these tools were developed decades ago with a strong base of statistical methods \cite{Cressie2011, Finkenstadt2006}, which are still used with success today. With the advent of machine learning, and deep learning techniques in particular, the ability to process voluminous data is becoming more accurate and efficient, but still at a considerably high computational cost in processing and computation. % TODO\cite{}.

In this work, we propose a methodology to perform forecast at scale in a spatio-temporal domain represented by univariate time series. We integrate tools designed for three types of knowledge fields: i) time series clustering and classification; b) time series analysis and forecasting; and c) processing spatio--temporal predictive queries. In this chapter, we review the literature related to different methods and techniques for these tasks, also discussing their applicability which influenced our decisions to select among the available methods. Considering that many works have already been referenced in the previous chapter, these next sections are dedicated to exploring related works in each of these three research areas.

\section{Time Series Clustering and Classification}
\label{Sec:ClusteringRelatedWorks}

Clustering provides a mechanism to automatically find some structure in large data sets that would be otherwise difficult to summarize or visualize \cite{Aggarwal2013}. Clustering time series and other sequences of data has become an important topic, motivated by several research challenges, including the development of methods to recognize dynamic change in time series \cite{Wang2004}. 

For the $k$-means clustering algorithm, a core element is distance calculation. Considering every time series as a 1D array of observations, then the similarity among two time series could be computed using simple proximity measures such as the Euclidean distance and the correlation strength, which consider a one-to-one correspondence between the elements of the two arrays. However, sometimes it is the case that two similar time series are not exactly aligned with one another but show the same pattern of activity over time \cite{Fu2011}. The Dynamic Time Warping (DTW) \cite{Sakoe1978} measure is able to capture such forms of similarity among time series. 

One drawback for the $k$-means algorithm when applied to time series clustering is that the obtained prototype may not actually function properly as a representative that captures the temporal trends of the members of its group \cite{Aghabozorgi2015}. Here, the $k$--Medoids approach is available as a variant where the prototype for each cluster (medoid), has a time series that is part of the dataset. %The application of $k$-Medoids with DTW is widely used, in \cite{} 
% TODO also try and mention other works that have used kmedoids without DTW
The next few paragraphs will focus on other works that have leveraged the clustering of multimedia time series data using $k$--Medoids, as well as the DTW measure.

\subsection{Clustering Time Series Data Using Dynamic Time Warping}

The authors in \cite{Ruiz2020} explored the use of $k$--Means, $k$--Medoids, Hierarchical clustering and Gaussian Mixtures, among other algorihtms, to try and obtain the best grouping of time-series for the application of intelligent systems in the energy efficiency field. Their objective is the acquisition of non-trivial knowledge in energy-related raw data, by means of an automatic strategy to search and analyze energy periodicity in these series recursively. Their results indicate that $k$--Medoids with PAM outperforms other alternatives, also that the squared Euclidean distance was the best choice for their data. In our work, we validate $k$--Medoids as an appropriate algorithm for our dataset, but we shifted our focus away from the Euclidean distance in favor of DTW; this because the former failed to capture temporal misalignments that are present in our dataset.

Another work with relevant application of time-series clustering \cite{Nakagawa2019}, now in the area of empirical finance, focused on various methods to predict future stock prices. It leveraged both $k$--Medoids and the Indexing Dynamic Time Warping (IDTW) method to capture price fluctuation patterns and to effectively predict monthly stock price changes. While our work used a standard implementation of the DTW measure quite effectively, we recognize that the indexing problem is an important performance aspect that may become more relevant as we work with larger datasets.

In the field of bioinformatics, the Dynamic Programming (DP) approach for sequence alignment is being used extensively \cite{Dinov2016}. In particular, the DTW measure as a DP tool has been applied, along with $k$--Medoids, in the area of Electro Cardiogram (ECG) heart-beat analysis with noteworthy success \cite{Annam2011}. The objective is to identify abnormalities in the ECG heart-beats through clustering, so this work also analyzes univariate time series data. An important difference with our approach is that the authors are looking to categorize heart-beat sequences into known categories, for which they can measure the accuracy of the clustering. Instead, we set out to find unknown groups so that we later use them to evaluate predictive accuracy of models built on representatives.

\subsection{On Time Series Classification}

Like any standard supervised classifiers, time series classification (TSC) algorithms build a classifier from a collection of labeled time series data \cite{Bagnall2017a}. Earlier works on TSC involve classifiers such as the Support Vector Machines [48] and Decision Trees [49, 50]. Some fit a model to each time series by assuming that time series in a class are generated by an underlying model or probability distribution [51]. Generally speaking, some widely used models include the Naive Bayes [52], Hidden Markov Model [53] and some Auto-Regressive models. But as mentioned in [23], most of these models are not fit for TSC analysis as they were proposed for other tasks, for example linear and non-linear regressions. 
% TODO floro de por que auto-regressive puede servir

In recent years, Deep learning \cite{Charniak2019, Goodfellow2016} has gained a lot of interest in the machine learning community. It has been very successful in many classification tasks such as computer vision [70, 71], natural language processing (NLP) [72, 73] and speech recognition [74, 75]. These successes have sparked further interest in developing deep learning models for TSC [68, 76, 77]. In fact, many of applications, such as speech recognition and NLP, have been determined to share similarities with TSC tasks [68].

This work is not focused on deep learning methods, thus in this section we only briefly review some of the deep learning architectures used for TSC. This review is mainly based on the the works from \cite{Bagnall2017a, Fawaz2019} both to date the more complete for Deep Learning for TSC.

One of the earliest works in deep learning for TSC is [78], where a Multi-scale Convolutional Neural Network (MCNN) is proposed. MCNN is relevant to the state-of-the-art of TSC classifiers COTE [78]. However, the method is complicated to deploy because it requires some heavy data preprocessing such as down-sampling and sliding windows, to prepare for the multi-scale setting.

Multi Layer Perceptron (MLP), Fully Convolutional Networks (FCN) and Residual Network (ResNet) are proposed for TSC in [76]. Their experiments show that FCN and ResNet are very competitive against MCNN and other state-of-the-art approaches such as COTE and BOSS. On the other hand, the proposed MLP is competitive with NN-DTW. More importantly, these models have small training and deploying complexity compared to MCNN and the ensemble-based classifiers. In contrast to the results in [76], a recent review of existing deep learning models [68] show that ResNet outperforms FCN when evaluated on 85 benchmark time series datasets [6] instead of the original 44 [76].

% TODO LSTM? Hybrid?

\section{Uni-Variate Time Series Analysis}
\label{Sec:TSAnalysis}
% Review the process of modeling spatiotemporal data, focusing in simple methods that are very expensive 
Predicting the future state of a phenomenon can be regarded as a time series modeling and forecasting problem. According to the number of independent variables for univariate time series, the current value of the series is only related to its historical values \cite{Hyndman2006}, this is a reasonable assumption for common time series data such as financial data and temperature data.

% Modelos para problemas espacio-temporales:
Techniques for statistical modeling and spectral analysis of real or complex-valued time series have been in use for more than five decades \cite{Hyndman2006, Chatfield2019}. Weather forecasting, financial or stock market prediction, and automatic process control are some of the oldest and most studied applications of such time series analysis \cite{Box1976}. These applications saw the advent of an increased role for machine learning techniques like Hidden Markov Models and time-delay neural networks in time series analysis \cite{}.

With regard to the type of models, time series analysis problems can be divided into linear and nonlinear models. Traditional linear time series modeling include autoregression (AR), moving average (MA), and autoregression moving average (ARMA) \cite{Chatfield2001, Murat2018}. Nonlinear modeling includes autoregressive conditional heteroskedasticity (ARCH) \cite{Kirchgassner2008} and general autoregressive conditional heteroskedasticity (GARCH) \cite{Kirchgassner2008}. In our work, we will review the applicability of some of these traditional linear methods; but we will not focus on models based on heteroskedasticity as they are a better fit for financial modeling.

Our decision for considering ARIMA models is based on its success cases for various applications such as medicine, business, economics, finance and engineering. Moreover, ARIMA models have become a major tool in numerous meteorological applications in order to understand the phenomena of air temperature and precipitation \cite{Murat2018}. The works revisited show that the linear ARIMA model and the quadratic ARIMA model had the best overall performance in making short-term predictions of annual absolute temperature in Libya and Iran for a 50-year time period \cite{ElMallah2016}. ARIMA models are also being used to  predict the temperature and precipitation in Afyonkarahisar Province, Turkey, until the year 2025, and found an increase in temperature according to the quadratic and linear trend models \cite{Muhammet2012}. In a study for the statistical properties of historical temperature data in Canada, the seasonal ARIMA model was found to be effective at predicting future temperature records for the period 1913-2013 \cite{Khedhiri2016}.

Approaches based on hybrid techniques can be found in the field of energy consumption, where short-term  load becomes relevant \cite{Nie2012}. Normally this variable is affected by many factors and is difficult to forecast accurately with a single model. The authors take advantage of the autoregressive integrated moving average (ARIMA) to forecast the linear basic part of load and of the support vector machines (SVMs) to forecast the non-linear sensitive part of load. It firstly uses ARIMA to forecast the daily load, and then uses SVMs,  which  is  known  for  the  great  power  to  learn  and  generalize,  to  correct  the  deviation  of  former  forecasting. Their results show that the hybrid model achieves expected forecasting  accuracy and has very good prospects for similar applications.

The authors in \cite{Shahriari2020} recognize the limitation of parametric models such as ARIMA in achieving high prediction accuracy on applications such as traffic volume prediction. They propose a bootstrap apparoch to obtain an ensemble of ARIMA models (E-ARIMA), where each model is developed using a random subsample of data. Using a real dataset for the traffic flow of Sydney, Australia, they demonstrate that their proposal improves the prediction accuracy of a purely ARIMA-based approach. For this thesis, we consider working only with ARIMA models, but our proposal of considering several representatives and respective predictive models, produces a similar effect as an ensemble, that is superior to a single model applied to the entire dataset.

\section{Executing Spatio-Temporal Predictive Queries}
\label{Sec:RelatedWorksQueries}

The common uses for predictive queries in spatio-temporal data are the support for predictive analytics to answer complex questions involving missing or future values, correlations, and trends, which can be used to identify opportunities or threats \cite{}. The predictive functionality can help build introspective services that assist in various data and resource management and optimization tasks (offline or online predictive techniques).

A spatio-temporal predictive query, particularly a predictive range query, has a query region $R$ and a time $t$ and asks about the predictions expected to be inside $R$ after time $t$ based on historical data (or previous knowledge). 

In \cite{Akdere2011}, the authors discuss the integration or extension of a RDBMS with a predictive component that can support data-driven predictive analytics. A predictive query and spatio-temporal predictive query is defined as a traditional query using a declarative language with a predictive capability \cite{Hendawi2012}. 

The framework Clipper \cite{Crankshaw2017} is designed to serve trained models at interactive latency. It implements two model selection policies based on multi-armed bandit algorithms. Both policies span a trade-off between accuracy and computation overhead with adaptable batch sizes. Rafiki \cite{Wang2018} is a machine learning training and inference service. It provides an online multi-model selection to compose ensembles for multiple requests. Rafiki uses a reinforcement learning approach to reward accuracy and penalize overdue requests. 

Willump \cite{Kraft2019}, improves ML inference performance by leveraging differing query modalities. Assuming ML models are used in higher-level end-to-end user queries in an ML application (compute the top-K predictions for a recommendation model) a query-aware adaptive parallelization.

While our work does not tackle directly all the considerations behind an implementation of a Predictive Serving System, we include some works that support these systems (such as Clipper, Rafiki, Prophet). As a result, we can better understand the requirements behind model composition and model selection, as well as sheding light on how our methodology can be implemented within a Predictive Serving System to serve these purposes in the future.