\chapter{Related Works}
\label{chapter_Related_Works}

Spatio-temporal phenomena are present in almost every field of research, such as climate science \cite{Faghmous2014, Guevara2020}, geographic information systems (GIS) \cite{ElGeresy2002, Shekhar2015}, epidemiology \cite{Malchow2007, Cabrera2019}, among others. With the advancement of technology to acquire data for these phenomena, it is possible to store massive amounts of spatio-temporal data \cite{Atluri2018}. In this work, we are interested in the representation of a given phenomenon using univariate time series, such as temperature \cite{Muhammet2012, Romilly2005}, energy consumption \cite{Abdelaal2008}, or econometrics \cite{Moral2003}.

Analyzing and studying space-time phenomena through data is a complex task \cite{Rao2008}, and the case of univariate time series is not the exception. There are several tools available, depending on the type of analysis and information to be extracted. Some of these tools were developed decades ago with a strong base of statistical methods \cite{Cressie2011, Finkenstadt2006}, which are still used with success today. With the advent of machine learning, and deep learning techniques in particular, the ability to process voluminous data is becoming more accurate and efficient, but still at a considerably high computational cost in processing and computation \cite{Yang2020, Karim2018}.

A predictive serving system \cite{Crankshaw2018} enables users to express queries that specify: a spatio-temporal region $R$, a target variable, and an evaluation metric; and asks about the predictions expected to be inside $R$ after time $t$ based on historical data (or previous knowledge). The common uses for spatio-temporal predictive queries in spatio-temporal data are the support for predictive analytics to answer complex questions involving missing or future values, correlations, and trends, which can be used to identify opportunities or threats \cite{Ghanta022019, Polyzotis2018}. The predictive functionality can help build introspective services that assist in various data and resource management and optimization tasks (offline or online predictive techniques) \cite{DeFilippo2018}.



In this work, we propose a methodology to perform forecast at scale in a spatio-temporal domain represented by univariate time series. We integrate tools designed for three types of knowledge fields: i) time series clustering and classification; b) time series analysis and forecasting; and c) processing spatio-temporal predictive queries. In this chapter, we review the literature related to different methods and techniques for these tasks, also discussing their applicability which influenced our decisions to select among the available methods. Considering that many works have already been referenced in the previous chapter, these next sections are dedicated to exploring related works in each of these three research areas.

\section{Time Series Clustering and Classification}
\label{Sec:ClusteringRelatedWorks}

Clustering provides a mechanism to automatically find some structure in large data sets that would be otherwise difficult to summarize or visualize \cite{Aggarwal2013}. Clustering time series and other sequences of data has become an important topic, motivated by several research challenges, including the development of methods to recognize dynamic change in time series \cite{Wang2004}. 

For the $k$-means clustering algorithm, a core element is distance calculation. Considering every time series as a 1D array of observations, then the similarity among two time series could be computed using simple proximity measures such as the Euclidean distance and the correlation strength, which consider a one-to-one correspondence between the elements of the two arrays. However, sometimes it is the case that two similar time series are not exactly aligned with one another but show the same pattern of activity over time \cite{Fu2011}. The Dynamic Time Warping (DTW) \cite{Sakoe1978} measure is able to capture such forms of similarity among time series. 

One drawback for the $k$-means algorithm when applied to time series clustering is that the obtained prototype may not actually function properly as a representative that captures the temporal trends of the members of its group \cite{Aghabozorgi2015}. Here, the $k$--Medoids approach is available as a variant where the prototype for each cluster (medoid), has a time series that is part of the dataset. %The application of $k$-Medoids with DTW is widely used, in \cite{} 
% TODO also try and mention other works that have used kmedoids without DTW
The next few paragraphs will focus on other works that have leveraged the clustering of multimedia time series data using $k$--Medoids, as well as the DTW measure.

\subsection{Clustering Time Series Data Using Dynamic Time Warping}

The authors in \cite{Ruiz2020} explored the use of $k$--Means, $k$--Medoids, Hierarchical clustering and Gaussian Mixtures, among other algorithms, to try and obtain the best grouping of time-series for the application of intelligent systems in the energy efficiency field. Their objective is the acquisition of non-trivial knowledge in energy-related raw data, by means of an automatic strategy to search and analyze energy periodicity in these series recursively. Their results indicate that $k$--Medoids with PAM outperforms other alternatives, also that the squared Euclidean distance was the best choice for their data. In our work, we validate $k$--Medoids as an appropriate algorithm for our dataset, but we shifted our focus away from the Euclidean distance in favor of DTW; this because the former failed to capture temporal misalignment that are present in our dataset.

Another work with relevant application of time-series clustering \cite{Nakagawa2019}, now in the area of empirical finance, focused on various methods to predict future stock prices. It leveraged both $k$--Medoids and the Indexing Dynamic Time Warping (IDTW) method to capture price fluctuation patterns and to effectively predict monthly stock price changes. While our work used a standard implementation of the DTW measure quite effectively, we recognize that the indexing problem is an important performance aspect that may become more relevant as we work with larger datasets.

In the field of bioinformatics, the Dynamic Programming (DP) approach for sequence alignment is being used extensively \cite{Dinov2016}. In particular, the DTW measure as a DP tool has been applied, along with $k$--Medoids, in the area of Electro-Cardiogram (ECG) heart-beat analysis with noteworthy success \cite{Annam2011}. The objective is to identify abnormalities in the ECG heart-beats through clustering, so this work also analyzes univariate time series data. An important difference with our approach is that the authors are looking to categorize heart-beat sequences into known categories, for which they can measure the accuracy of the clustering. Instead, we set out to find unknown groups so that we later use them to evaluate predictive accuracy of models built on representatives.

\subsection{On Time Series Classification}

Time series classification (TSC) algorithms build a classifier from a collection of labeled time series data \cite{Bagnall2017a}. Earlier works on TSC involve classifiers such as the Support Vector Machines \cite{Kampouraki2008, Mitsa2010} and Decision Trees \cite{Douzal2012, Mitsa2010}. 

The application of Neural Network algorithms to solve TSC is becoming very popular and one of its advantages is that it is resistant to noise. The input layer consists of the attributes used in the classification, and the output nodes correspond to the classes \cite{Mitsa2010}. In recent years, Deep learning \cite{Charniak2019, Goodfellow2016} has gained a lot of interest in the machine learning community. It has been very successful in many classification tasks such as computer vision \cite{Krizhevsky2017}, natural language processing (NLP) \cite{Bahdanau2015} and speech recognition \cite{Hinton2012}. These successes have sparked further interest in developing deep learning models for TSC \cite{Wang2017, Fawaz2019}. In fact, many of applications, such as speech recognition and NLP, have been determined to share similarities with TSC tasks \cite{Fawaz2019}.

This review is not focused on deep learning methods, we only briefly review some of the deep learning architectures used for TSC. This review is mainly based on an exploratory survey \cite{Bagnall2017a} and a thesis aimed at deep learning for TSC \cite{Fawaz2019}. Also, worth mentioning is the site \url{http://www.timeseriesclassification.com}, which represents an effort to reunite dataset and research papers for new studies on this evolving topic.

An important advantage of deep neural networks are its potential ability to perform time series classification without heavy pre-processing of the raw data or feature crafting, as shown by \cite{Wang2017}. This is possible using a Fully Convolutional Network (FCN) with global average pooling, a complex solution which leverages the Class Activation Map (CAM) to uncover labels from the raw data. In the context of our proposed methodology, our classifier is based on a one-dimensional convolutional neural network-long short-term memory (1D CNN-LSTM) model. A solution based on FCN is considered an end-to-end approach that requires the specification of several parameters and hyper-parameters; we deemed that the level of complexity of 1D CNN-LSTM model was enough to provide satisfactory results for our dataset, as validated by our computational experiments.

% Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline \cite{Wang2017}
% Convolutional neural networks for time series classification \cite{Zhao2017}

A similar application of the 1D CNN-LSTM model for TSC can be found in the field of electroencephalographic (EEG) signal recordings \cite{Xu2020}. This work explains the appropriateness of the approach by considering the contribution of the layers of the hybrid approach: after pre-processing the time series, the 1D CNN is used to extract features of the normalized sequence data; these features are processed by the LSTM to further capture temporal features. These features are then passed on to fully connected layers for classification of epileptic seizures. A very similar approach guided our implementation to classify time series of elements in a domain into known labels that are then mapped one-to-one to representatives, in order to use their corresponding predictive models on the time series.

%A One-Dimensional CNN-LSTM Model for Epileptic Seizure Recognition Using EEG Signal Analysis \cite{Xu2020}
%Frequent epileptic seizures cause damage to the human brain, resulting in memory impairment, mental decline, and so on. Therefore, it is important to detect epileptic seizures and provide medical treatment in a timely manner. Currently, medical experts recognize epileptic seizure activity through the visual inspection of electroencephalographic (EEG) signal recordings of patients based on their experience, which takes much time and effort. In view of this, this paper proposes a one-dimensional convolutional neural network-long short-term memory (1D CNN-LSTM) model for automatic recognition of epileptic seizures through EEG signal analysis. Firstly, the raw EEG signal data are pre-processed and normalized. Then, a 1D convolutional neural network (CNN) is designed to effectively extract the features of the normalized EEG sequence data. In addition, the extracted features are then processed by the LSTM layers in order to further extract the temporal features. After that, the output features are fed into several fully connected layers for final epileptic seizure recognition. The performance of the proposed 1D CNN-LSTM model is verified on the public UCI epileptic seizure recognition data set. Experiments results show that the proposed method achieves high recognition accuracies of 99.39% and 82.00% on the binary and five-class epileptic seizure recognition tasks, respectively. Comparing results with traditional machine learning methods including k-nearest neighbors, support vector machines, and decision trees, other deep learning methods including standard deep neural network and CNN further verify the superiority of the proposed method.


CNNs, LSTMs and DNNs are individually limited in their modeling capabilities, an overview of the modeling limitations of RNNs (and thus LSTMs) is provided in \cite{Pascanu2014}. One issue with LSTMs is that the temporal modeling is done on the input feature. However, higher-level modeling of the input feature can help to extract underlying factors of variation within the input, which should then make it easier to learn temporal structure between successive time steps \cite{Pascanu2014}.  %In the following works we review works that consider hybrid architectures for TSC.

%Attention-based LSTM-CNNs For Time-series Classification \cite{Qianjin2018}
%ime series classification is a critical problem in the machine learning field, which spawns numerous research works on it. In this work, we propose AttLSTM-CNNs, an attention-based LSTM network and convolution network that jointly extracts the underlying pattern among the time-series for the classification. The attention-based LSTM automatically captures the long-term temporal dependency among the series, and the CNN describes the spatial sparsity and heterogeneity in the data. The extensive experiments show that the proposed model outperforms the other methods for time-series classification.

\section{Uni-Variate Time Series Analysis}
\label{Sec:TSAnalysis}
% Review the process of modeling spatiotemporal data, focusing in simple methods that are very expensive 
Predicting the future state of a phenomenon can be regarded as a time series modeling and forecasting problem. According to the number of independent variables for univariate time series, the current value of the series is only related to its historical values \cite{Hyndman2006}, this is a reasonable assumption for common time series data such as financial data and temperature data.

% Modelos para problemas espacio-temporales:
Techniques for statistical modeling and spectral analysis of real or complex-valued time series have been in use for more than five decades \cite{Hyndman2006, Chatfield2019}. Weather forecasting, financial or stock market prediction, and automatic process control are some of the oldest and most studied applications of such time series analysis \cite{Box1976}. These applications saw the advent of an increased role for machine learning techniques like Hidden Markov Models and time-delay neural networks in time series analysis \cite{}.

With regard to the type of models, time series analysis problems can be divided into linear and nonlinear models. Traditional linear time series modeling include autoregression (AR), moving average (MA), and autoregression moving average (ARMA) \cite{Chatfield2001, Murat2018}. Nonlinear modeling includes autoregressive conditional heteroskedasticity (ARCH) \cite{Kirchgassner2008} and general autoregressive conditional heteroskedasticity (GARCH) \cite{Kirchgassner2008}. In our work, we will review the applicability of some of these traditional linear methods; but we will not focus on models based on heteroskedasticity as they are a better fit for financial modeling.

Our decision for considering ARIMA models is based on its success cases for various applications such as medicine, business, economics, finance and engineering. Moreover, ARIMA models have become a major tool in numerous meteorological applications in order to understand the phenomena of air temperature and precipitation \cite{Murat2018}. The works revisited show that the linear ARIMA model and the quadratic ARIMA model had the best overall performance in making short-term predictions of annual absolute temperature in Libya and Iran for a 50-year time period \cite{ElMallah2016}. ARIMA models are also being used to  predict the temperature and precipitation in Afyonkarahisar Province, Turkey, until the year 2025, and found an increase in temperature according to the quadratic and linear trend models \cite{Muhammet2012}. In a study for the statistical properties of historical temperature data in Canada, the seasonal ARIMA model was found to be effective at predicting future temperature records for the period 1913-2013 \cite{Khedhiri2016}.

Approaches based on hybrid techniques can be found in the field of energy consumption, where short-term  load becomes relevant \cite{Nie2012}. Normally this variable is affected by many factors and is difficult to forecast accurately with a single model. The authors take advantage of the autoregressive integrated moving average (ARIMA) to forecast the linear basic part of load and of the support vector machines (SVMs) to forecast the non-linear sensitive part of load. It firstly uses ARIMA to forecast the daily load, and then uses SVMs,  which  is  known  for  the  great  power  to  learn  and  generalize,  to  correct  the  deviation  of  former  forecasting. Their results show that the hybrid model achieves expected forecasting  accuracy and has very good prospects for similar applications.

The authors in \cite{Shahriari2020} recognize the limitation of parametric models such as ARIMA in achieving high prediction accuracy on applications such as traffic volume prediction. They propose a bootstrap apparoch to obtain an ensemble of ARIMA models (E-ARIMA), where each model is developed using a random subsample of data. Using a real dataset for the traffic flow of Sydney, Australia, they demonstrate that their proposal improves the prediction accuracy of a purely ARIMA-based approach. For this thesis, we consider working only with ARIMA models, but our proposal of considering several representatives and respective predictive models, produces a similar effect as an ensemble, that is superior to a single model applied to the entire dataset.

\section{Executing Spatio-Temporal Predictive Queries}
\label{Sec:RelatedWorksQueries}

The common uses for predictive queries in spatio-temporal data are the support for predictive analytics to answer complex questions involving missing or future values, correlations, and trends, which can be used to identify opportunities or threats \cite{}. The predictive functionality can help build introspective services that assist in various data and resource management and optimization tasks (offline or online predictive techniques).

A spatio-temporal predictive query, particularly a predictive range query, has a query region $R$ and a time $t$ and asks about the predictions expected to be inside $R$ after time $t$ based on historical data (or previous knowledge). 

In \cite{Akdere2011}, the authors discuss the integration or extension of a RDBMS with a predictive component that can support data-driven predictive analytics. A predictive query and spatio-temporal predictive query is defined as a traditional query using a declarative language with a predictive capability \cite{Hendawi2012}. 

The framework Clipper \cite{Crankshaw2017} is designed to serve trained models at interactive latency. It implements two model selection policies based on multi-armed bandit algorithms. Both policies span a trade-off between accuracy and computation overhead with adaptable batch sizes. Rafiki \cite{Wang2018} is a machine learning training and inference service. It provides an online multi-model selection to compose ensembles for multiple requests. Rafiki uses a reinforcement learning approach to reward accuracy and penalize overdue requests. 

Willump \cite{Kraft2019}, improves ML inference performance by leveraging differing query modalities. Assuming ML models are used in higher-level end-to-end user queries in an ML application (compute the top-K predictions for a recommendation model) a query-aware adaptive parallelization.

While our work does not tackle directly all the considerations behind an implementation of a Predictive Serving System, we include some works that support these systems (such as Clipper, Rafiki, Prophet). As a result, we can better understand the requirements behind model composition and model selection, as well as sheding light on how our methodology can be implemented within a Predictive Serving System to serve these purposes in the future.